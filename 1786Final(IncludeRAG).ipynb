{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "api_key = os.getenv('OPENAI_API_KEY')\n",
        "if api_key:\n",
        "    print(\"OPENAI_API_KEY is set:\", api_key)\n",
        "else:\n",
        "    print(\"OPENAI_API_KEY is not set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fbPclVATQ5K",
        "outputId": "42cf4a40-2746-4dc4-9112-a163a43ea1c6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OPENAI_API_KEY is set: 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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index\n",
        "!pip install llama-index-embeddings-openai\n",
        "!pip install llama-index-llms-openai\n",
        "!pip install llama-index-readers-file\n",
        "!pip install docx2txt\n",
        "!pip install llama_index.retrievers.bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "K3lOkxhuTudy",
        "outputId": "6b53967d-1f5d-4942-bc40-c74e00011a96"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.12.2)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.6.3)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.9.48.post4)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.3.2)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.3.0)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.3.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.54.4)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.2->llama-index) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (3.11.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (1.2.15)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (11.0.0)\n",
            "Requirement already satisfied: pydantic<2.10.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (1.16.0)\n",
            "Requirement already satisfied: llama-cloud>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.12.3)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.1.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.5.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (0.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.7.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.2->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.2->llama-index) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.2->llama-index) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.2->llama-index) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.2->llama-index) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.2->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.2->llama-index) (3.23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (1.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.2->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
            "Requirement already satisfied: llama-index-embeddings-openai in /usr/local/lib/python3.10/dist-packages (0.3.1)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-openai) (0.12.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-openai) (1.54.4)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.11.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.2.15)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (11.0.0)\n",
            "Requirement already satisfied: pydantic<2.10.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.16.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.23.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (24.2)\n",
            "Requirement already satisfied: llama-index-llms-openai in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-openai) (0.12.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-openai) (1.54.4)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (3.11.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (1.2.15)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (11.0.0)\n",
            "Requirement already satisfied: pydantic<2.10.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (1.16.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->llama-index-llms-openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->llama-index-llms-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (3.23.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai) (24.2)\n",
            "Requirement already satisfied: llama-index-readers-file in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file) (4.12.3)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file) (0.12.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file) (5.1.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file) (0.0.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.6)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (3.11.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.2.15)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (11.0.0)\n",
            "Requirement already satisfied: pydantic<2.10.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (4.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (3.23.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (24.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.2.2)\n",
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.10/dist-packages (0.8)\n",
            "Requirement already satisfied: llama_index.retrievers.bm25 in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: bm25s<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama_index.retrievers.bm25) (0.2.5)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from llama_index.retrievers.bm25) (0.12.2)\n",
            "Requirement already satisfied: pystemmer<3.0.0.0,>=2.2.0.1 in /usr/local/lib/python3.10/dist-packages (from llama_index.retrievers.bm25) (2.2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bm25s<0.3.0,>=0.2.0->llama_index.retrievers.bm25) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bm25s<0.3.0,>=0.2.0->llama_index.retrievers.bm25) (1.26.4)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (3.11.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (1.2.15)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (3.9.1)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (11.0.0)\n",
            "Requirement already satisfied: pydantic<2.10.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (4.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (3.23.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (24.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.retrievers.bm25) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NG1pkdfaTD5A"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, StorageContext, ServiceContext, SimpleDirectoryReader, Document\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "from llama_index.core.storage.docstore.simple_docstore import SimpleDocumentStore\n",
        "from llama_index.core.vector_stores.simple import SimpleVectorStore, DEFAULT_VECTOR_STORE\n",
        "from llama_index.core.graph_stores.simple import SimpleGraphStore\n",
        "from llama_index.core.storage.index_store.simple_index_store import SimpleIndexStore\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "from llama_index.core.retrievers import QueryFusionRetriever\n",
        "from llama_index.core import Settings\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import openai\n",
        "import json\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.core.evaluation import SemanticSimilarityEvaluator\n",
        "# evaluator = SemanticSimilarityEvaluator()"
      ],
      "metadata": {
        "id": "Db7kvCz86mpo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "6Xx2XLWyecvv",
        "outputId": "472eff78-6ff8-42ea-bcf7-ae0c29498055"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f833d3ff-ee8a-4298-805e-285dada4d4a8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f833d3ff-ee8a-4298-805e-285dada4d4a8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving combined_json.json to combined_json.json\n",
            "Saving processed_job_descriptions.json to processed_job_descriptions (2).json\n",
            "Saving processed_original_resumes.json to processed_original_resumes (2).json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Json File\n",
        "import json\n",
        "from openai import OpenAI\n",
        "\n",
        "with open('processed_job_descriptions.json', \"r\") as file:\n",
        "    input_jd = json.load(file)\n",
        "\n",
        "with open('processed_original_resumes.json', \"r\") as file:\n",
        "    input_resume = json.load(file)\n",
        "\n",
        "with open('combined_json.json', \"r\") as file:\n",
        "    raginformation = json.load(file)"
      ],
      "metadata": {
        "id": "uu8paiXUocCz"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# 1. Combine Resume and JD for Each Profile\n",
        "\n",
        "documents = [\n",
        "    Document(\n",
        "        text=f\"Resume: {profile['resume']} JobDescription: {profile['job_description']}\",\n",
        "        extra_info={\n",
        "            \"job_title\": profile[\"job_title\"],\n",
        "            \"resume\": profile[\"resume\"],\n",
        "            \"job_description\": profile[\"job_description\"]\n",
        "        }\n",
        "    )\n",
        "    for profile in raginformation\n",
        "]\n",
        "\n",
        "# 2. Initialize Required Stores\n",
        "docstore = SimpleDocumentStore()\n",
        "vector_store = SimpleVectorStore()\n",
        "graph_store = SimpleGraphStore()\n",
        "index_store = SimpleIndexStore()\n",
        "\n",
        "# 3. Initialize StorageContext\n",
        "storage_context = StorageContext(\n",
        "    docstore=docstore,\n",
        "    index_store=index_store,\n",
        "    #vector_stores=vector_store,\n",
        "    vector_stores={DEFAULT_VECTOR_STORE: vector_store},\n",
        "    graph_store=graph_store\n",
        ")\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import Settings\n",
        "Settings.llm = OpenAI(temperature=0.1, model=\"gpt-4\")\n",
        "#Settings.llm = OpenAI()\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
        "Settings.node_parser = SentenceSplitter(chunk_size=2048, chunk_overlap=20)\n",
        "Settings.context_window = 3900\n",
        "\n",
        "\n",
        "# 4. Create VectorStoreIndex\n",
        "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, transformations=[SentenceSplitter(chunk_size=2048)])\n"
      ],
      "metadata": {
        "id": "Pmu-WuTjcxxc"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Extract Resume Indices\n",
        "resume_indices = [resume[\"ResumeIndex\"] for resume in input_resume[\"Original Resumes\"]]\n",
        "\n",
        "# Extract Job Description Indices\n",
        "jd_indices = [jd[\"JDIndex\"] for jd in input_jd[\"JobDescriptions\"]]\n",
        "\n",
        "# Form all combinations of Resume and JD indices\n",
        "pairs = list(itertools.product(resume_indices, jd_indices))\n",
        "# Retrieve paired data\n",
        "paired_data = []\n",
        "for resume_idx, jd_idx in pairs:\n",
        "    resume_data = input_resume[\"Original Resumes\"][resume_idx][\"ResumeJSON\"]\n",
        "    jd_data = input_jd[\"JobDescriptions\"][jd_idx][\"JDJSON\"]\n",
        "    paired_data.append({\"Resume\": resume_data, \"JobDescription\": jd_data})\n",
        "\n",
        "# Example: Print first paired data\n",
        "print(paired_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDkimyQ3Cxdm",
        "outputId": "c7698692-dd31-48f9-aa83-dde4fce1fab0"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Resume': {'Education': 'Honors Bachelor of Science in Physics, Minor in Computing\\nInstitution: University of Waterloo', 'Skills': 'Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), SQL, Java, JavaScript (including jQuery)\\nMachine Learning: Regression, SVM, Nave Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, NLP, Dimensionality Reduction, Topic Modeling (LDA, NMF), PCA, Neural Networks\\nDatabase & Visualization Tools: MySQL, SQL Server, Cassandra, HBase, Elasticsearch, D3.js, DC.js, Plotly, Kibana, matplotlib, ggplot, Tableau\\nOther Tools/Technologies: Regular Expressions, HTML, CSS, Angular 6, Logstash, Kafka, Flask, Git, Docker, OpenCV, Deep Learning concepts', 'Experience': 'Data Science Assurance Associate  Ernst & Young LLP\\nFraud Investigations & Dispute Services\\nTechnology Assisted Review (TAR):\\nDeveloped an automated review platform implementing predictive coding and topic modeling, reducing review costs and time.\\nConducted R&D on classification models, predictive analysis, and text data mining for fraud detection.\\nTools: Python, scikit-learn, tfidf, word2vec, doc2vec, Nave Bayes, LDA, NMF, Tableau\\nMultiple Data Science Projects for USA Clients:\\nText Analytics (Motor Vehicle Customer Reviews):\\nPerformed sentiment analysis and time-series analysis on survey data.\\nCreated visualizations like heat maps, word clouds, and Tableau dashboards.\\nTools: Python, NLP (NLTK, spacy), scikit-learn, Tableau\\nChatbot Development:\\nDesigned a chatbot to handle customer queries, build question pipelines, and provide recommendations.\\nTools: Python, JavaScript, SQL Server, NLP, topic modeling\\nInformation Governance:\\nScanned and analyzed unstructured data for metadata extraction and indexing in Elasticsearch.\\nConducted ROT (Redundant, Outdated, Trivial) analysis and full-text search for identifying PII.\\nTools: Python, Flask, Elasticsearch, Kibana\\nFraud Analytic Platform (FAP):\\nDeveloped a platform for fraud detection using advanced analytics for ERP systems.\\nTools: HTML, JavaScript, SQL Server, jQuery, Bootstrap, D3.js, DC.js', 'Projects': ''}, 'JobDescription': {'Role': 'Design, build, and maintain highly scalable, robust, and efficient cloud infrastructure using Google Cloud Platform (GCP) services, including Vertex AI, BigTable, BigQuery, and Cloud Composer. Develop automation and orchestration of ML pipelines, integrating data ingestion, feature engineering, training, and deployment processes. Collaborate with cross-functional teams to understand their needs and build solutions that improve platform usability, scalability, and the overall development experience. Optimize data processing pipelines and cloud resources to ensure low-latency, cost-effective operation. Implement monitoring, alerting, and failover strategies to ensure platform reliability. Stay updated with industry trends and best practices in cloud engineering, data engineering, and machine learning', 'Qualification': 'Customer-centric mindset: Passionate about delivering an exceptional experience for data scientists through a self-service platform, reducing friction in their workflows. Collaboration: Strong communication skills to work closely with cross-functional teams, including data scientists and engineers, to ensure platform features meet user needs and expectations. Problem-solving: Ability to identify and solve complex technical issues related to ML pipelines, cloud infrastructure, and scalability, ensuring an efficient and robust platform. Automation-first approach: Commitment to streamlining and automating processes for scalability and reliability, enabling data scientists to focus on experimentation and model development. Adaptability: Ability to quickly adjust to new technologies and evolving platform needs to keep the infrastructure cutting-edge and efficient. Ownership and initiative: Comfortable taking ownership of key platform components, driving innovation and improvements that benefit the platforms scalability and usability. Bachelors or Masters degree in Computer Science, Engineering, or a related field. 2+ years of experience in software engineering with a focus on cloud infrastructure and/or data engineering. Hands-on experience with Google Cloud Platform services such as Vertex AI, BigTable, BigQuery, Cloud Composer, Cloud Storage, etc. Proficiency in one or more programming languages such as Python, Java, and SQL. Experience with orchestration tools such as Apache Airflow (Composer). Knowledge of CI/CD pipelines and DevOps tools for continuous integration and deployment. Familiarity with containerization and orchestration (Docker, Kubernetes). Strong problem-solving skills and attention to detail. Excellent communication skills and ability to work in a collaborative, fast-paced environment', 'Title': 'SDE'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh6xYLNInKX8",
        "outputId": "a0521ff0-7cac-454e-9fc8-19da26bd791c"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 0),\n",
              " (0, 1),\n",
              " (0, 2),\n",
              " (1, 0),\n",
              " (1, 1),\n",
              " (1, 2),\n",
              " (2, 0),\n",
              " (2, 1),\n",
              " (2, 2),\n",
              " (3, 0),\n",
              " (3, 1),\n",
              " (3, 2),\n",
              " (4, 0),\n",
              " (4, 1),\n",
              " (4, 2),\n",
              " (5, 0),\n",
              " (5, 1),\n",
              " (5, 2),\n",
              " (6, 0),\n",
              " (6, 1),\n",
              " (6, 2),\n",
              " (7, 0),\n",
              " (7, 1),\n",
              " (7, 2),\n",
              " (8, 0),\n",
              " (8, 1),\n",
              " (8, 2),\n",
              " (9, 0),\n",
              " (9, 1),\n",
              " (9, 2),\n",
              " (10, 0),\n",
              " (10, 1),\n",
              " (10, 2),\n",
              " (11, 0),\n",
              " (11, 1),\n",
              " (11, 2),\n",
              " (12, 0),\n",
              " (12, 1),\n",
              " (12, 2),\n",
              " (13, 0),\n",
              " (13, 1),\n",
              " (13, 2),\n",
              " (14, 0),\n",
              " (14, 1),\n",
              " (14, 2),\n",
              " (15, 0),\n",
              " (15, 1),\n",
              " (15, 2),\n",
              " (16, 0),\n",
              " (16, 1),\n",
              " (16, 2),\n",
              " (17, 0),\n",
              " (17, 1),\n",
              " (17, 2),\n",
              " (18, 0),\n",
              " (18, 1),\n",
              " (18, 2),\n",
              " (19, 0),\n",
              " (19, 1),\n",
              " (19, 2),\n",
              " (20, 0),\n",
              " (20, 1),\n",
              " (20, 2),\n",
              " (21, 0),\n",
              " (21, 1),\n",
              " (21, 2),\n",
              " (22, 0),\n",
              " (22, 1),\n",
              " (22, 2),\n",
              " (23, 0),\n",
              " (23, 1),\n",
              " (23, 2),\n",
              " (24, 0),\n",
              " (24, 1),\n",
              " (24, 2),\n",
              " (25, 0),\n",
              " (25, 1),\n",
              " (25, 2),\n",
              " (26, 0),\n",
              " (26, 1),\n",
              " (26, 2),\n",
              " (27, 0),\n",
              " (27, 1),\n",
              " (27, 2),\n",
              " (28, 0),\n",
              " (28, 1),\n",
              " (28, 2),\n",
              " (29, 0),\n",
              " (29, 1),\n",
              " (29, 2),\n",
              " (30, 0),\n",
              " (30, 1),\n",
              " (30, 2),\n",
              " (31, 0),\n",
              " (31, 1),\n",
              " (31, 2),\n",
              " (32, 0),\n",
              " (32, 1),\n",
              " (32, 2),\n",
              " (33, 0),\n",
              " (33, 1),\n",
              " (33, 2),\n",
              " (34, 0),\n",
              " (34, 1),\n",
              " (34, 2),\n",
              " (35, 0),\n",
              " (35, 1),\n",
              " (35, 2),\n",
              " (36, 0),\n",
              " (36, 1),\n",
              " (36, 2),\n",
              " (37, 0),\n",
              " (37, 1),\n",
              " (37, 2),\n",
              " (38, 0),\n",
              " (38, 1),\n",
              " (38, 2),\n",
              " (39, 0),\n",
              " (39, 1),\n",
              " (39, 2),\n",
              " (40, 0),\n",
              " (40, 1),\n",
              " (40, 2),\n",
              " (41, 0),\n",
              " (41, 1),\n",
              " (41, 2),\n",
              " (42, 0),\n",
              " (42, 1),\n",
              " (42, 2),\n",
              " (43, 0),\n",
              " (43, 1),\n",
              " (43, 2),\n",
              " (44, 0),\n",
              " (44, 1),\n",
              " (44, 2),\n",
              " (45, 0),\n",
              " (45, 1),\n",
              " (45, 2),\n",
              " (46, 0),\n",
              " (46, 1),\n",
              " (46, 2),\n",
              " (47, 0),\n",
              " (47, 1),\n",
              " (47, 2),\n",
              " (48, 0),\n",
              " (48, 1),\n",
              " (48, 2),\n",
              " (49, 0),\n",
              " (49, 1),\n",
              " (49, 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_text_list(paired_data):\n",
        "    combined_total = []\n",
        "    for data in paired_data:\n",
        "        resume_data = data[\"Resume\"]\n",
        "        jd_data = data[\"JobDescription\"]\n",
        "            # Combine resume and job description text\n",
        "        combined_text = (\n",
        "            f\"Education: {resume_data.get('Education', '')}\\n\"\n",
        "            f\"Skills: {resume_data.get('Skills', '')}\\n\"\n",
        "            f\"Experience: {resume_data.get('Experience', '')}\\n\"\n",
        "            f\"Projects: {resume_data.get('Projects', '')}\\n\\n\"\n",
        "            f\"Role: {jd_data.get('Role', '')}\\n\"\n",
        "            f\"Qualification: {jd_data.get('Qualification', '')}\\n\"\n",
        "            f\"Title: {jd_data.get('Title', '')}\\n\"\n",
        "        )\n",
        "        combined_total.append(combined_text)\n",
        "    return combined_total\n",
        "combined_text_list = combine_text_list(paired_data[:9])"
      ],
      "metadata": {
        "id": "L0aa7Fx7k3gR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(combined_text_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC3d4qLxnbBi",
        "outputId": "1822b3ab-1ef9-4e9f-d115-5a297bbe5047"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# index_mapping = {\n",
        "#     0: 0,  # Combine ResumeIndex 0 with JDIndex 0\n",
        "# }\n",
        "\n",
        "# # Combine and generate embeddings\n",
        "# combined_data = []\n",
        "# for resume_index, jd_index in index_mapping.items():\n",
        "#     # Retrieve resume and job description\n",
        "#     resume_data = next(r for r in input_resume[\"Original Resumes\"] if r[\"ResumeIndex\"] == resume_index)[\"ResumeJSON\"]\n",
        "#     jd_data = next(j for j in input_jd[\"JobDescriptions\"] if j[\"JDIndex\"] == jd_index)[\"JDJSON\"]\n",
        "\n",
        "#     # Combine resume and job description text\n",
        "#     combined_text = (\n",
        "#         f\"Education: {resume_data.get('Education', '')}\\n\"\n",
        "#         f\"Skills: {resume_data.get('Skills', '')}\\n\"\n",
        "#         f\"Experience: {resume_data.get('Experience', '')}\\n\"\n",
        "#         f\"Projects: {resume_data.get('Projects', '')}\\n\\n\"\n",
        "#         f\"Role: {jd_data.get('Role', '')}\\n\"\n",
        "#         f\"Qualification: {jd_data.get('Qualification', '')}\\n\"\n",
        "#         f\"Title: {jd_data.get('Title', '')}\\n\"\n",
        "#     )\n",
        "# combined_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "Aflha0BSDaZf",
        "outputId": "28dfea8a-0ecf-45ef-9b64-0927b631334a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Education: Honors Bachelor of Science in Physics, Minor in Computing\\nInstitution: University of Waterloo\\nSkills: Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), SQL, Java, JavaScript (including jQuery)\\nMachine Learning: Regression, SVM, Nave Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, NLP, Dimensionality Reduction, Topic Modeling (LDA, NMF), PCA, Neural Networks\\nDatabase & Visualization Tools: MySQL, SQL Server, Cassandra, HBase, Elasticsearch, D3.js, DC.js, Plotly, Kibana, matplotlib, ggplot, Tableau\\nOther Tools/Technologies: Regular Expressions, HTML, CSS, Angular 6, Logstash, Kafka, Flask, Git, Docker, OpenCV, Deep Learning concepts\\nExperience: Data Science Assurance Associate  Ernst & Young LLP\\nFraud Investigations & Dispute Services\\nTechnology Assisted Review (TAR):\\nDeveloped an automated review platform implementing predictive coding and topic modeling, reducing review costs and time.\\nConducted R&D on classification models, predictive analysis, and text data mining for fraud detection.\\nTools: Python, scikit-learn, tfidf, word2vec, doc2vec, Nave Bayes, LDA, NMF, Tableau\\nMultiple Data Science Projects for USA Clients:\\nText Analytics (Motor Vehicle Customer Reviews):\\nPerformed sentiment analysis and time-series analysis on survey data.\\nCreated visualizations like heat maps, word clouds, and Tableau dashboards.\\nTools: Python, NLP (NLTK, spacy), scikit-learn, Tableau\\nChatbot Development:\\nDesigned a chatbot to handle customer queries, build question pipelines, and provide recommendations.\\nTools: Python, JavaScript, SQL Server, NLP, topic modeling\\nInformation Governance:\\nScanned and analyzed unstructured data for metadata extraction and indexing in Elasticsearch.\\nConducted ROT (Redundant, Outdated, Trivial) analysis and full-text search for identifying PII.\\nTools: Python, Flask, Elasticsearch, Kibana\\nFraud Analytic Platform (FAP):\\nDeveloped a platform for fraud detection using advanced analytics for ERP systems.\\nTools: HTML, JavaScript, SQL Server, jQuery, Bootstrap, D3.js, DC.js\\nProjects: \\n\\nRole: Design, build, and maintain highly scalable, robust, and efficient cloud infrastructure using Google Cloud Platform (GCP) services, including Vertex AI, BigTable, BigQuery, and Cloud Composer. Develop automation and orchestration of ML pipelines, integrating data ingestion, feature engineering, training, and deployment processes. Collaborate with cross-functional teams to understand their needs and build solutions that improve platform usability, scalability, and the overall development experience. Optimize data processing pipelines and cloud resources to ensure low-latency, cost-effective operation. Implement monitoring, alerting, and failover strategies to ensure platform reliability. Stay updated with industry trends and best practices in cloud engineering, data engineering, and machine learning\\nQualification: Customer-centric mindset: Passionate about delivering an exceptional experience for data scientists through a self-service platform, reducing friction in their workflows. Collaboration: Strong communication skills to work closely with cross-functional teams, including data scientists and engineers, to ensure platform features meet user needs and expectations. Problem-solving: Ability to identify and solve complex technical issues related to ML pipelines, cloud infrastructure, and scalability, ensuring an efficient and robust platform. Automation-first approach: Commitment to streamlining and automating processes for scalability and reliability, enabling data scientists to focus on experimentation and model development. Adaptability: Ability to quickly adjust to new technologies and evolving platform needs to keep the infrastructure cutting-edge and efficient. Ownership and initiative: Comfortable taking ownership of key platform components, driving innovation and improvements that benefit the platforms scalability and usability. Bachelors or Masters degree in Computer Science, Engineering, or a related field. 2+ years of experience in software engineering with a focus on cloud infrastructure and/or data engineering. Hands-on experience with Google Cloud Platform services such as Vertex AI, BigTable, BigQuery, Cloud Composer, Cloud Storage, etc. Proficiency in one or more programming languages such as Python, Java, and SQL. Experience with orchestration tools such as Apache Airflow (Composer). Knowledge of CI/CD pipelines and DevOps tools for continuous integration and deployment. Familiarity with containerization and orchestration (Docker, Kubernetes). Strong problem-solving skills and attention to detail. Excellent communication skills and ability to work in a collaborative, fast-paced environment\\nTitle: SDE\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_top_profiles(combined_text, index, similarity_top_k=3, vector_weight=0.6, bm25_weight=0.4):\n",
        "    \"\"\"\n",
        "    Retrieves the top profiles based on combined BM25 and VectorStore retrievers.\n",
        "\n",
        "    Args:\n",
        "        combined_text (str): The input text to retrieve relevant profiles for.\n",
        "        index (VectorStoreIndex): The index to use for retrieval.\n",
        "        similarity_top_k (int): Number of top similar profiles to retrieve.\n",
        "        vector_weight (float): Weight for the vector retriever.\n",
        "        bm25_weight (float): Weight for the BM25 retriever.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries containing top profiles with metadata.\n",
        "    \"\"\"\n",
        "    # Initialize retrievers\n",
        "    bm25_retriever = BM25Retriever.from_defaults(\n",
        "        docstore=index.docstore, similarity_top_k=similarity_top_k\n",
        "    )\n",
        "    vector_retriever = index.as_retriever(similarity_top_k=similarity_top_k)\n",
        "\n",
        "    # Combine retrievers using QueryFusionRetriever\n",
        "    retriever = QueryFusionRetriever(\n",
        "        retrievers=[vector_retriever, bm25_retriever],\n",
        "        retriever_weights=[vector_weight, bm25_weight],\n",
        "        similarity_top_k=similarity_top_k,\n",
        "        num_queries=1,  # Disable query generation\n",
        "        mode=\"dist_based_score\",  # Use distance-based score mode\n",
        "        use_async=False,  # Synchronous mode\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    # Retrieve nodes with scores\n",
        "    nodes_with_scores = retriever.retrieve(combined_text)\n",
        "\n",
        "    # Process the retrieved results\n",
        "    top_profiles = []\n",
        "    for node in nodes_with_scores:\n",
        "        extra_info = node.node.extra_info  # Metadata from the document\n",
        "        top_profiles.append({\n",
        "            \"job_title\": extra_info.get(\"job_title\", \"N/A\"),\n",
        "            \"resume\": extra_info.get(\"resume\", \"N/A\"),\n",
        "            \"job_description\": extra_info.get(\"job_description\", \"N/A\"),\n",
        "            # \"similarity_score\": node.score,  # Uncomment if you want to include scores\n",
        "        })\n",
        "\n",
        "    return top_profiles\n"
      ],
      "metadata": {
        "id": "za3Fj5RcIFgZ"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize BM25 and Vector Retrievers\n",
        "# bm25_retriever = BM25Retriever.from_defaults(\n",
        "#     docstore=index.docstore, similarity_top_k=3\n",
        "# )\n",
        "# vector_retriever = index.as_retriever(similarity_top_k=3)\n",
        "\n",
        "# # Combine retrievers using QueryFusionRetriever\n",
        "# retriever = QueryFusionRetriever(\n",
        "#     retrievers=[vector_retriever, bm25_retriever],\n",
        "#     retriever_weights=[0.6, 0.4],  # Adjust weights as needed\n",
        "#     similarity_top_k=2,  # Retrieve top 5 results\n",
        "#     num_queries=1,  # Disable query generation\n",
        "#     mode=\"dist_based_score\",  # Use distance-based score mode\n",
        "#     use_async=False,  # Synchronous mode\n",
        "#     verbose=True,\n",
        "# )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgXphUyfLkw_",
        "outputId": "aa19e6ce-50cd-42b2-8c7e-ab7c22effddd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:bm25s:Building index from IDs objects\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nodes_with_scores = retriever.retrieve(combined_text_list[0])\n",
        "\n",
        "# # Process the retrieved results\n",
        "# top_profiles = []\n",
        "# for node in nodes_with_scores:\n",
        "#     extra_info = node.node.extra_info  # Metadata from the document\n",
        "#     top_profiles.append({\n",
        "#         \"job_title\": extra_info[\"job_title\"],\n",
        "#         \"resume\": extra_info[\"resume\"],\n",
        "#         \"job_description\": extra_info[\"job_description\"],\n",
        "#        # \"similarity_score\": node.score,\n",
        "#     })\n",
        "\n",
        "# # Display the Top 5 Results\n",
        "# import pprint\n",
        "# pprint.pprint(top_profiles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC7r0s1wDMaX",
        "outputId": "c7c68efd-15c3-45da-dbbd-6e8abb649bc5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'job_description': {'Qualification': 'Customer-centric mindset: Passionate '\n",
            "                                       'about delivering an exceptional '\n",
            "                                       'experience for data scientists through '\n",
            "                                       'a self-service platform, reducing '\n",
            "                                       'friction in their workflows. '\n",
            "                                       'Collaboration: Strong communication '\n",
            "                                       'skills to work closely with '\n",
            "                                       'cross-functional teams, including data '\n",
            "                                       'scientists and engineers, to ensure '\n",
            "                                       'platform features meet user needs and '\n",
            "                                       'expectations. Problem-solving: Ability '\n",
            "                                       'to identify and solve complex '\n",
            "                                       'technical issues related to ML '\n",
            "                                       'pipelines, cloud infrastructure, and '\n",
            "                                       'scalability, ensuring an efficient and '\n",
            "                                       'robust platform. Automation-first '\n",
            "                                       'approach: Commitment to streamlining '\n",
            "                                       'and automating processes for '\n",
            "                                       'scalability and reliability, enabling '\n",
            "                                       'data scientists to focus on '\n",
            "                                       'experimentation and model development. '\n",
            "                                       'Adaptability: Ability to quickly '\n",
            "                                       'adjust to new technologies and '\n",
            "                                       'evolving platform needs to keep the '\n",
            "                                       'infrastructure cutting-edge and '\n",
            "                                       'efficient. Ownership and initiative: '\n",
            "                                       'Comfortable taking ownership of key '\n",
            "                                       'platform components, driving '\n",
            "                                       'innovation and improvements that '\n",
            "                                       'benefit the platforms scalability and '\n",
            "                                       'usability. Bachelors or Masters '\n",
            "                                       'degree in Computer Science, '\n",
            "                                       'Engineering, or a related field. 2+ '\n",
            "                                       'years of experience in software '\n",
            "                                       'engineering with a focus on cloud '\n",
            "                                       'infrastructure and/or data '\n",
            "                                       'engineering. Hands-on experience with '\n",
            "                                       'Google Cloud Platform services such as '\n",
            "                                       'Vertex AI, BigTable, BigQuery, Cloud '\n",
            "                                       'Composer, Cloud Storage, etc. '\n",
            "                                       'Proficiency in one or more programming '\n",
            "                                       'languages such as Python, Java, and '\n",
            "                                       'SQL. Experience with orchestration '\n",
            "                                       'tools such as Apache Airflow '\n",
            "                                       '(Composer). Knowledge of CI/CD '\n",
            "                                       'pipelines and DevOps tools for '\n",
            "                                       'continuous integration and deployment. '\n",
            "                                       'Familiarity with containerization and '\n",
            "                                       'orchestration (Docker, Kubernetes). '\n",
            "                                       'Strong problem-solving skills and '\n",
            "                                       'attention to detail. Excellent '\n",
            "                                       'communication skills and ability to '\n",
            "                                       'work in a collaborative, fast-paced '\n",
            "                                       'environment',\n",
            "                      'Role': 'Design, build, and maintain highly scalable, '\n",
            "                              'robust, and efficient cloud infrastructure '\n",
            "                              'using Google Cloud Platform (GCP) services, '\n",
            "                              'including Vertex AI, BigTable, BigQuery, and '\n",
            "                              'Cloud Composer. Develop automation and '\n",
            "                              'orchestration of ML pipelines, integrating data '\n",
            "                              'ingestion, feature engineering, training, and '\n",
            "                              'deployment processes. Collaborate with '\n",
            "                              'cross-functional teams to understand their '\n",
            "                              'needs and build solutions that improve platform '\n",
            "                              'usability, scalability, and the overall '\n",
            "                              'development experience. Optimize data '\n",
            "                              'processing pipelines and cloud resources to '\n",
            "                              'ensure low-latency, cost-effective operation. '\n",
            "                              'Implement monitoring, alerting, and failover '\n",
            "                              'strategies to ensure platform reliability. Stay '\n",
            "                              'updated with industry trends and best practices '\n",
            "                              'in cloud engineering, data engineering, and '\n",
            "                              'machine learning'},\n",
            "  'job_title': 'SDE',\n",
            "  'resume': {'Education': 'Princeton, NJ Princeton University Sept 2013-June '\n",
            "                          '2017 Major: Electrical Engineering, B.S.E (in-major '\n",
            "                          'GPA: 3.44). Certificate (Minor): Computer Science. '\n",
            "                          'Programming Coursework: Algorithms & Data '\n",
            "                          'Structures, Operating Systems, Networks, Computer '\n",
            "                          'Vision. EE Coursework: Embedded Systems, IoT, '\n",
            "                          'Computer Arch., Circuits, Logic Design, VLSI '\n",
            "                          'Design, Signal Processing.',\n",
            "             'Experience': 'Firmware Engineer, Intern Stryd (startup) June-Aug '\n",
            "                           '2016 Foot pod (www.stryd.com): Wearable Power '\n",
            "                           \"Meter For Running. Improved device's battery \"\n",
            "                           'lifespan by 8% by integrating a fuel gauge sensor '\n",
            "                           'and establishing a battery saving state. Utilized '\n",
            "                           'the I2C protocol to implement a device driver for '\n",
            "                           'the fuel gauge and used it to create a low power '\n",
            "                           'state. Increased available flash memory by 66% '\n",
            "                           'through redesigning the flash data storage system '\n",
            "                           'with a circular buffer implementation that '\n",
            "                           'supported variable-sized records. Leveraged '\n",
            "                           'knowledge in Git, ARM Cortex-M4 architecture, '\n",
            "                           'programmed in C using Keil IDE, and debugged using '\n",
            "                           'an Oscilloscope, Multimeter, Memory Analyzer, and '\n",
            "                           'JTAG/SWD debugging interface. Software Developer, '\n",
            "                           'Intern Autodesk June-Aug 2015 TinkerCad '\n",
            "                           '(www.tinkercad.com): online 3D design and printing '\n",
            "                           'tool. Integrated multi-touch gestures for 3D '\n",
            "                           'workspaces by creating a deterministic finite '\n",
            "                           'state machine for HTML events. Implemented a '\n",
            "                           'low-pass and smoothing function to allow for a '\n",
            "                           'user-friendly touch experience. Established remote '\n",
            "                           'testing and coding development environment using '\n",
            "                           'Docker and bash scripts. Leveraged knowledge in '\n",
            "                           'Full Stack Web development, JavaScript, Git, and '\n",
            "                           'debugged using Chrome Developer Tools.',\n",
            "             'Projects': 'Personal Website: www.terrencekuo.com (for '\n",
            "                         'additional information and projects). iOS Meme App: '\n",
            "                         'Developed an iOS application using Swift and '\n",
            "                         'Objective-C that allows users to easily create and '\n",
            "                         'share memes. Integrated openCV library allowing '\n",
            "                         'users to effortlessly apply photo filters and '\n",
            "                         'effects. Incorporated persistent data storage to '\n",
            "                         'archive memes. Leveraged caching for recently '\n",
            "                         'accessed memes. Designed RESTful backend server '\n",
            "                         'enabling memes to be stored persistently in an '\n",
            "                         'online database. Utilized: Swift, Obj-C, Local '\n",
            "                         'Persistent Data, Caching, Cloud Storage, Python, '\n",
            "                         'Flask, SQLite, openCV. Autonomous RC Car + Virtual '\n",
            "                         'Driving: Designed and implemented PID speed control '\n",
            "                         'for an RC car by constructing a Hall effect circuit '\n",
            "                         'to measure speed and a PWM motor controller circuit '\n",
            "                         'to control speed. Added autonomous driving by '\n",
            "                         'constructing an image processing circuit and '\n",
            "                         'implementing PID steering control. Created a '\n",
            "                         'virtual driving experience by manufacturing a '\n",
            "                         'gimbal mount and creating an iOS app that wirelessly '\n",
            "                         'displays and operates the cameras FOV and direction. '\n",
            "                         'The app also remotely controls speed and steering. '\n",
            "                         'Utilized: C programming, PSoC, Socket (IP/TCP) '\n",
            "                         'Programming, O-scope, Multimeter, Arduino, Web & iOS '\n",
            "                         'Dev. Home Automation: Temperature Sensor with '\n",
            "                         'Android Interface: Created an Android App that '\n",
            "                         'bit-banged BeagleBones I2C module to read '\n",
            "                         'temperature data off the DS1621 digital thermometer '\n",
            "                         'sensor and visualized temperature changes. Utilized: '\n",
            "                         'C programming, BeagleBone Microcontroller, '\n",
            "                         'Oscilloscope, Circuit Design, Android Development. '\n",
            "                         'Real-Time Interactive 3D-Graphics Website '\n",
            "                         '(http://interactive-graphics.herokuapp.com): '\n",
            "                         'Developed an interactive graphics website using '\n",
            "                         'THREE.js to create a 3D workspace with real-time '\n",
            "                         'animated 3D models of crystal lattice structures and '\n",
            "                         'robotic parts in which animations and camera views '\n",
            "                         'can be manipulated. Inspired from struggling with '\n",
            "                         'visualizing 3D models while taking a materials '\n",
            "                         'science class. Utilized: Python, Flask, Heroku, '\n",
            "                         'JavaScript, AJAX, THREE.js, HTML/CSS, Docker, GIT.',\n",
            "             'Skills': 'Software: (proficient): C, Python, Swift, Unix, Git '\n",
            "                       '(familiar): Java, C++, Go, SQL, Matlab, JavaScript, '\n",
            "                       'HTML/CSS.'}},\n",
            " {'job_description': {'Qualification': 'Customer-centric mindset: Passionate '\n",
            "                                       'about delivering an exceptional '\n",
            "                                       'experience for data scientists through '\n",
            "                                       'a self-service platform, reducing '\n",
            "                                       'friction in their workflows. '\n",
            "                                       'Collaboration: Strong communication '\n",
            "                                       'skills to work closely with '\n",
            "                                       'cross-functional teams, including data '\n",
            "                                       'scientists and engineers, to ensure '\n",
            "                                       'platform features meet user needs and '\n",
            "                                       'expectations. Problem-solving: Ability '\n",
            "                                       'to identify and solve complex '\n",
            "                                       'technical issues related to ML '\n",
            "                                       'pipelines, cloud infrastructure, and '\n",
            "                                       'scalability, ensuring an efficient and '\n",
            "                                       'robust platform. Automation-first '\n",
            "                                       'approach: Commitment to streamlining '\n",
            "                                       'and automating processes for '\n",
            "                                       'scalability and reliability, enabling '\n",
            "                                       'data scientists to focus on '\n",
            "                                       'experimentation and model development. '\n",
            "                                       'Adaptability: Ability to quickly '\n",
            "                                       'adjust to new technologies and '\n",
            "                                       'evolving platform needs to keep the '\n",
            "                                       'infrastructure cutting-edge and '\n",
            "                                       'efficient. Ownership and initiative: '\n",
            "                                       'Comfortable taking ownership of key '\n",
            "                                       'platform components, driving '\n",
            "                                       'innovation and improvements that '\n",
            "                                       'benefit the platforms scalability and '\n",
            "                                       'usability. Bachelors or Masters '\n",
            "                                       'degree in Computer Science, '\n",
            "                                       'Engineering, or a related field. 2+ '\n",
            "                                       'years of experience in software '\n",
            "                                       'engineering with a focus on cloud '\n",
            "                                       'infrastructure and/or data '\n",
            "                                       'engineering. Hands-on experience with '\n",
            "                                       'Google Cloud Platform services such as '\n",
            "                                       'Vertex AI, BigTable, BigQuery, Cloud '\n",
            "                                       'Composer, Cloud Storage, etc. '\n",
            "                                       'Proficiency in one or more programming '\n",
            "                                       'languages such as Python, Java, and '\n",
            "                                       'SQL. Experience with orchestration '\n",
            "                                       'tools such as Apache Airflow '\n",
            "                                       '(Composer). Knowledge of CI/CD '\n",
            "                                       'pipelines and DevOps tools for '\n",
            "                                       'continuous integration and deployment. '\n",
            "                                       'Familiarity with containerization and '\n",
            "                                       'orchestration (Docker, Kubernetes). '\n",
            "                                       'Strong problem-solving skills and '\n",
            "                                       'attention to detail. Excellent '\n",
            "                                       'communication skills and ability to '\n",
            "                                       'work in a collaborative, fast-paced '\n",
            "                                       'environment',\n",
            "                      'Role': 'Design, build, and maintain highly scalable, '\n",
            "                              'robust, and efficient cloud infrastructure '\n",
            "                              'using Google Cloud Platform (GCP) services, '\n",
            "                              'including Vertex AI, BigTable, BigQuery, and '\n",
            "                              'Cloud Composer. Develop automation and '\n",
            "                              'orchestration of ML pipelines, integrating data '\n",
            "                              'ingestion, feature engineering, training, and '\n",
            "                              'deployment processes. Collaborate with '\n",
            "                              'cross-functional teams to understand their '\n",
            "                              'needs and build solutions that improve platform '\n",
            "                              'usability, scalability, and the overall '\n",
            "                              'development experience. Optimize data '\n",
            "                              'processing pipelines and cloud resources to '\n",
            "                              'ensure low-latency, cost-effective operation. '\n",
            "                              'Implement monitoring, alerting, and failover '\n",
            "                              'strategies to ensure platform reliability. Stay '\n",
            "                              'updated with industry trends and best practices '\n",
            "                              'in cloud engineering, data engineering, and '\n",
            "                              'machine learning'},\n",
            "  'job_title': 'SDE',\n",
            "  'resume': {'Education': 'Princeton, NJ Princeton University Sept 2013-June '\n",
            "                          '2017 Major: Electrical Engineering, B.S.E (in-major '\n",
            "                          'GPA: 3.44). Certificate (Minor): Computer Science. '\n",
            "                          'Programming Coursework: Algorithms & Data '\n",
            "                          'Structures, Operating Systems, Networks, Computer '\n",
            "                          'Vision. EE Coursework: Embedded Systems, IoT, '\n",
            "                          'Computer Arch., Circuits, Logic Design, VLSI '\n",
            "                          'Design, Signal Processing.',\n",
            "             'Experience': 'Firmware Engineer, Intern Stryd (startup) June-Aug '\n",
            "                           '2016 Foot pod (www.stryd.com): Wearable Power '\n",
            "                           \"Meter For Running. Improved device's battery \"\n",
            "                           'lifespan by 8% by integrating a fuel gauge sensor '\n",
            "                           'and establishing a battery saving state. Utilized '\n",
            "                           'the I2C protocol to implement a device driver for '\n",
            "                           'the fuel gauge and used it to create a low power '\n",
            "                           'state. Increased available flash memory by 66% '\n",
            "                           'through redesigning the flash data storage system '\n",
            "                           'with a circular buffer implementation that '\n",
            "                           'supported variable-sized records. Leveraged '\n",
            "                           'knowledge in Git, ARM Cortex-M4 architecture, '\n",
            "                           'programmed in C using Keil IDE, and debugged using '\n",
            "                           'an Oscilloscope, Multimeter, Memory Analyzer, and '\n",
            "                           'JTAG/SWD debugging interface. Software Developer, '\n",
            "                           'Intern Autodesk June-Aug 2015 TinkerCad '\n",
            "                           '(www.tinkercad.com): online 3D design and printing '\n",
            "                           'tool. Integrated multi-touch gestures for 3D '\n",
            "                           'workspaces by creating a deterministic finite '\n",
            "                           'state machine for HTML events. Implemented a '\n",
            "                           'low-pass and smoothing function to allow for a '\n",
            "                           'user-friendly touch experience. Established remote '\n",
            "                           'testing and coding development environment using '\n",
            "                           'Docker and bash scripts. Leveraged knowledge in '\n",
            "                           'Full Stack Web development, JavaScript, Git, and '\n",
            "                           'debugged using Chrome Developer Tools.',\n",
            "             'Projects': 'Personal Website: www.terrencekuo.com (for '\n",
            "                         'additional information and projects). iOS Meme App: '\n",
            "                         'Developed an iOS application using Swift and '\n",
            "                         'Objective-C that allows users to easily create and '\n",
            "                         'share memes. Integrated openCV library allowing '\n",
            "                         'users to effortlessly apply photo filters and '\n",
            "                         'effects. Incorporated persistent data storage to '\n",
            "                         'archive memes. Leveraged caching for recently '\n",
            "                         'accessed memes. Designed RESTful backend server '\n",
            "                         'enabling memes to be stored persistently in an '\n",
            "                         'online database. Utilized: Swift, Obj-C, Local '\n",
            "                         'Persistent Data, Caching, Cloud Storage, Python, '\n",
            "                         'Flask, SQLite, openCV. Autonomous RC Car + Virtual '\n",
            "                         'Driving: Designed and implemented PID speed control '\n",
            "                         'for an RC car by constructing a Hall effect circuit '\n",
            "                         'to measure speed and a PWM motor controller circuit '\n",
            "                         'to control speed. Added autonomous driving by '\n",
            "                         'constructing an image processing circuit and '\n",
            "                         'implementing PID steering control. Created a '\n",
            "                         'virtual driving experience by manufacturing a '\n",
            "                         'gimbal mount and creating an iOS app that wirelessly '\n",
            "                         'displays and operates the cameras FOV and direction. '\n",
            "                         'The app also remotely controls speed and steering. '\n",
            "                         'Utilized: C programming, PSoC, Socket (IP/TCP) '\n",
            "                         'Programming, O-scope, Multimeter, Arduino, Web & iOS '\n",
            "                         'Dev. Home Automation: Temperature Sensor with '\n",
            "                         'Android Interface: Created an Android App that '\n",
            "                         'bit-banged BeagleBones I2C module to read '\n",
            "                         'temperature data off the DS1621 digital thermometer '\n",
            "                         'sensor and visualized temperature changes. Utilized: '\n",
            "                         'C programming, BeagleBone Microcontroller, '\n",
            "                         'Oscilloscope, Circuit Design, Android Development. '\n",
            "                         'Real-Time Interactive 3D-Graphics Website '\n",
            "                         '(http://interactive-graphics.herokuapp.com): '\n",
            "                         'Developed an interactive graphics website using '\n",
            "                         'THREE.js to create a 3D workspace with real-time '\n",
            "                         'animated 3D models of crystal lattice structures and '\n",
            "                         'robotic parts in which animations and camera views '\n",
            "                         'can be manipulated. Inspired from struggling with '\n",
            "                         'visualizing 3D models while taking a materials '\n",
            "                         'science class. Utilized: Python, Flask, Heroku, '\n",
            "                         'JavaScript, AJAX, THREE.js, HTML/CSS, Docker, GIT.',\n",
            "             'Skills': 'Software: (proficient): C, Python, Swift, Unix, Git '\n",
            "                       '(familiar): Java, C++, Go, SQL, Matlab, JavaScript, '\n",
            "                       'HTML/CSS.'}}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "from llama_index.core.retrievers import QueryFusionRetriever\n",
        "\n",
        "def retrieve_top_profiles(index, combined_text, similarity_top_k=2, retriever_weights=[0.6, 0.4]):\n",
        "    \"\"\"\n",
        "    Retrieves the top profiles based on the combined_text using a QueryFusionRetriever.\n",
        "\n",
        "    Args:\n",
        "        index: The VectorStoreIndex object.\n",
        "        combined_text (str): The combined text containing resume and job description.\n",
        "        similarity_top_k (int): Number of top results to retrieve. Default is 2.\n",
        "        retriever_weights (list): Weights for combining vector and BM25 retrievers. Default is [0.6, 0.4].\n",
        "\n",
        "    Returns:\n",
        "        list: A list of top profiles with job title, resume, and job description.\n",
        "    \"\"\"\n",
        "    # Initialize BM25 and Vector Retrievers\n",
        "    bm25_retriever = BM25Retriever.from_defaults(\n",
        "        docstore=index.docstore, similarity_top_k=similarity_top_k\n",
        "    )\n",
        "    vector_retriever = index.as_retriever(similarity_top_k=similarity_top_k+5)\n",
        "\n",
        "    # Combine retrievers using QueryFusionRetriever\n",
        "    retriever = QueryFusionRetriever(\n",
        "        retrievers=[vector_retriever, bm25_retriever],\n",
        "        retriever_weights=retriever_weights,\n",
        "        similarity_top_k=similarity_top_k,\n",
        "        num_queries=1,  # Disable query generation\n",
        "        mode=\"dist_based_score\",  # Use distance-based score mode\n",
        "        use_async=False,  # Synchronous mode\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    # Retrieve nodes with scores\n",
        "    nodes_with_scores = retriever.retrieve(combined_text)\n",
        "\n",
        "    # Process the retrieved results\n",
        "    top_profiles = []\n",
        "    for node in nodes_with_scores:\n",
        "        extra_info = node.node.extra_info  # Metadata from the document\n",
        "        top_profiles.append({\n",
        "            \"job_title\": extra_info[\"job_title\"],\n",
        "            \"resume\": extra_info[\"resume\"],\n",
        "            \"job_description\": extra_info[\"job_description\"],\n",
        "        })\n",
        "\n",
        "    return top_profiles\n"
      ],
      "metadata": {
        "id": "nsfHzwLfoupQ"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_profiles = []\n",
        "for i in range(len(combined_text_list)):\n",
        "  profile = retrieve_top_profiles(index, combined_text_list[i],similarity_top_k=5)\n",
        "  top_profiles.append(profile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqKNxbNGJfB-",
        "outputId": "f1c866f3-5fdc-41b0-9c68-ec3bb69449ac"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:bm25s:Building index from IDs objects\n",
            "DEBUG:bm25s:Building index from IDs objects\n",
            "DEBUG:bm25s:Building index from IDs objects\n",
            "DEBUG:bm25s:Building index from IDs objects\n",
            "DEBUG:bm25s:Building index from IDs objects\n",
            "DEBUG:bm25s:Building index from IDs objects\n",
            "DEBUG:bm25s:Building index from IDs objects\n",
            "DEBUG:bm25s:Building index from IDs objects\n",
            "DEBUG:bm25s:Building index from IDs objects\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(top_profiles[0])"
      ],
      "metadata": {
        "id": "bpR-R8UZk0Vl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9455572-547c-4971-a50f-79c85322a29b"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ADVISOR_PERSONA = \"\"\"I am a highly experienced human resource with 15 years of specialized experience\n",
        "\n",
        "Primary role: Analyze the successful candidate profile and the corresponding job description and give the instructions step by step for revising the resume.\n",
        "\n",
        "# Guidelines for Providing Recommendations to Customers on Revising Their Resumes Based on Successful Candidate Profiles That Align Closely with Job Descriptions:\n",
        "1. Analysis of Successful Candidate Profile:\n",
        "    - Identify the key skills, qualifications, and experiences highlighted in the successful candidate's resume\n",
        "    - Note any specific industry terminology or jargon that aligns with the job description\n",
        "    - Pay attention to the formatting and structure of the resume, including sections that appear to be impactful\n",
        "\n",
        "2. Compare with Job Description:\n",
        "    - Examine the job description for essential skills, qualifications, and experiences that the employer is seeking\n",
        "    - Highlight any gaps or alignments between the successful candidate's profile and the job description\n",
        "\n",
        "3. Synthesize Findings:\n",
        "    - Summarize the key takeaways from the successful candidate profile that are relevant to the job description\n",
        "    - Identify the elements of the successful candidate's resume that should be emphasized or modified\n",
        "\n",
        "4. Give Instructions for Resume Revision:\n",
        "    - Based on the analysis, provide specific recommendations on how to adjust the resume to better align with the job description:\n",
        "    - Suggest adding relevant skills or experiences that are emphasized in the job description but missing in the profile\n",
        "    - Recommend changes in wording to incorporate industry terminology\n",
        "    - Give the instructions by each section of the successful candidate profile\n",
        "    - Detail practical steps or activities the candidate can take to develop the identified skills\n",
        "\n",
        "Note: Do not try to suggest adding new section of the original resume\n",
        "\n",
        "Note: Adapt these guidelines to each successful candidate profile and job description\n",
        "\n",
        "Note: Focus on instructions generation\n",
        "\"\"\"\n",
        "RESUME_WRITER_PERSONA = \"\"\"I am a highly experienced career advisor and resume writing expert with 15 years of specialized experience.\n",
        "\n",
        "Primary role: Craft exceptional resumes and cover letters tailored to specific job descriptions, optimized for both ATS systems and human readers.\n",
        "\n",
        "# Instructions for creating optimized resumes and cover letters\n",
        "1. Analyze job descriptions:\n",
        "   - Extract key requirements and keywords\n",
        "   - Note: Adapt analysis based on specific industry and role\n",
        "\n",
        "2. Create compelling resumes:\n",
        "   - Highlight quantifiable achievements (e.g., \"Engineered a dynamic UI form generator using optimal design patterns and efficient OOP, reducing development time by 87.5%\")\n",
        "   - Tailor content to specific job and company\n",
        "   - Emphasize candidate's unique value proposition\n",
        "\n",
        "3. Optimize for Applicant Tracking Systems (ATS):\n",
        "   - Use industry-specific keywords strategically throughout documents\n",
        "   - Ensure content passes ATS scans while engaging human readers\n",
        "\n",
        "4. Provide industry-specific guidance:\n",
        "   - Incorporate current hiring trends\n",
        "   - Prioritize relevant information (apply \"6-second rule\" for quick scanning)\n",
        "   - Use clear, consistent formatting\n",
        "\n",
        "5. Apply best practices:\n",
        "   - Quantify achievements where possible\n",
        "   - Use specific, impactful statements instead of generic ones\n",
        "   - Update content based on latest industry standards\n",
        "   - Use active voice and strong action verbs\n",
        "\n",
        "Note: Adapt these guidelines to each user's specific request, industry, and experience level.\n",
        "\n",
        "Goal: Create documents that not only pass ATS screenings but also compellingly demonstrate how the user can add immediate value to the prospective employer.\"\"\""
      ],
      "metadata": {
        "id": "ppBuvdAGnSKe"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# InstructionGeneratorprompt = \"\"\"\n",
        "# You are a career advisor specializing in resume optimization and career development. Your task is to analyze a candidate's\n",
        "# original resume, the job description for their target role, and a combined profile of successful candidate resumes with their\n",
        "# corresponding job descriptions. Based on these inputs, provide tailored and actionable career advice to help the candidate\n",
        "# improve their resume and advance their career.\n",
        "\n",
        "# ### Guidelines:\n",
        "# 1. **Analyze Successful Candidate Profiles**:\n",
        "#     - Identify key skills, qualifications, and experiences emphasized in successful profiles.\n",
        "#     - Note any specific industry terminology, tools, or methodologies highlighted.\n",
        "#     - Observe the formatting style and impactful sections in the successful profiles.\n",
        "\n",
        "# 2. **Compare Original Resume with Successful Profiles and Job Description**:\n",
        "#     - Highlight gaps or missing elements in the original resume compared to successful profiles and job description.\n",
        "#     - Identify transferable skills in the original resume that align with the target job requirements.\n",
        "\n",
        "# 3. **Provide Resume Revision Recommendations**:\n",
        "#     - Suggest specific updates for each section of the resume:\n",
        "#         - **Professional Summary**: Highlight key skills and achievements relevant to the job description.\n",
        "#         - **Skills**: Add or refine skills to better align with the successful profiles and job requirements.\n",
        "#         - **Work Experience**: Suggest rewording or adding achievements with measurable outcomes.\n",
        "#         - **Education and Certifications**: Recommend certifications, training, or coursework to strengthen the resume.\n",
        "#     - Use clear, actionable language, such as:\n",
        "#         - Add a bullet point showcasing your experience with [specific tool or methodology].\n",
        "#         - Rephrase your summary to include terms like [key industry terminology].\n",
        "\n",
        "# 4. **Provide Career Development Suggestions**:\n",
        "#     - Recommend activities or training programs to address skill gaps (e.g., certifications, courses, or networking).\n",
        "#     - Suggest practical ways to gain relevant experience, such as internships, volunteering, or projects.\n",
        "\n",
        "# ### Output Format:\n",
        "# 1. **Summary of Key Findings**:\n",
        "#     - Highlight major gaps and alignments between the original resume, successful profiles, and job description.\n",
        "\n",
        "# 2. **Step-by-Step Resume Revision Instructions**:\n",
        "#     - Provide actionable suggestions for improving each resume section.\n",
        "\n",
        "# 3. **Career Development Suggestions**:\n",
        "#     - Offer specific recommendations for upskilling, gaining experience, or enhancing qualifications.\n",
        "\n",
        "# Use the input data to generate tailored advice for the candidate.\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "GYWZBtTVbk9b"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "InstructionGeneratorPrompt = \"\"\"\n",
        "You are a career advisor specializing in resume optimization and career development. Your task is to analyze a candidate's\n",
        "original resume, the job description for their target role, and a combined profile of successful candidate resumes with their\n",
        "corresponding job descriptions. Based on these inputs, provide tailored and actionable career advice to help the candidate\n",
        "improve their resume and advance their career.\n",
        "\n",
        "### Guidelines:\n",
        "1. **Analyze Successful Candidate Profiles**:\n",
        "    - Identify key skills, qualifications, and experiences emphasized in successful profiles.\n",
        "    - Note any specific industry terminology, tools, or methodologies highlighted.\n",
        "    - Observe the formatting style and impactful sections in the successful profiles.\n",
        "\n",
        "2. **Compare Original Resume with Successful Profiles and Job Description**:\n",
        "    - Highlight gaps or missing elements in the original resume compared to successful profiles and job description.\n",
        "    - Identify transferable skills in the original resume that align with the target job requirements.\n",
        "\n",
        "3. **Provide Resume Revision Recommendations**:\n",
        "    - Suggest specific updates for each section of the resume:\n",
        "        - **Professional Summary**: Highlight key skills and achievements relevant to the job description.\n",
        "        - **Skills**: Add or refine skills to better align with the successful profiles and job requirements.\n",
        "        - **Work Experience**: Suggest rewording or adding achievements with measurable outcomes.\n",
        "        - **Education and Certifications**: Recommend certifications, training, or coursework to strengthen the resume.\n",
        "    - Use clear, actionable language, such as:\n",
        "        - Add a bullet point showcasing your experience with [specific tool or methodology].\n",
        "        - Rephrase your summary to include terms like [key industry terminology].\n",
        "\n",
        "4. **Provide Career Development Suggestions**:\n",
        "    - **Skill Gap Analysis**:\n",
        "        - Compare the candidate's current skills, qualifications, and experiences with those of the successful candidate(s)\n",
        "          to identify specific gaps or areas for improvement.\n",
        "        - Highlight any critical competencies or achievements the successful candidate possesses that are currently missing in the candidate's profile.\n",
        "    - **Actionable Recommendations**:\n",
        "        - Recommend activities or training programs tailored to close these gaps, such as:\n",
        "            - **Certifications**: Suggest relevant certifications (e.g., Google Cloud, AWS, Python, TensorFlow) to enhance technical expertise.\n",
        "            - **Courses**: Point to online or in-person courses (e.g., Coursera, edX, or LinkedIn Learning) for in-demand skills.\n",
        "            - **Networking**: Encourage participation in industry-specific conferences, webinars, or meetups to gain insights and build connections.\n",
        "    - **Experience Building**:\n",
        "        - Provide practical ways to gain relevant experience, such as:\n",
        "            - **Internships or Co-op Programs**: Suggest applying for roles that align with the desired career path.\n",
        "            - **Volunteer Work**: Recommend volunteering for organizations or projects requiring relevant expertise.\n",
        "            - **Freelance Projects**: Highlight opportunities to work on freelance or personal projects to demonstrate skill application.\n",
        "            - **Hackathons or Competitions**: Encourage participation in coding hackathons, data challenges, or similar competitions to showcase abilities.\n",
        "    - **Targeted Achievements**:\n",
        "        - Advise setting measurable short-term goals (e.g., \"Complete a machine learning certification within three months\") that align with achievements seen in successful candidates.\n",
        "        - Recommend specific projects or activities that mirror successful candidate profiles, such as contributing to open-source projects or presenting at industry conferences.\n",
        "    - **Resume Enhancement Activities**:\n",
        "        - Suggest ways to enhance their resume, such as:\n",
        "            - Writing articles, blogs, or tutorials related to their domain.\n",
        "            - Obtaining recommendations or endorsements on platforms like LinkedIn.\n",
        "            - Highlighting quantified achievements once skill gaps are addressed (e.g., \"Reduced processing time by XX%\" or \"Improved model accuracy by XX%\").\n",
        "\n",
        "### Output Format:\n",
        "1. **Summary of Key Findings**:\n",
        "    - Highlight major gaps and alignments between the original resume, successful profiles, and job description.\n",
        "\n",
        "2. **Step-by-Step Resume Revision Instructions**:\n",
        "    - Provide actionable suggestions for improving each resume section.\n",
        "\n",
        "3. **Career Development Suggestions**:\n",
        "    - Offer specific recommendations for upskilling, gaining experience, or enhancing qualifications.\n",
        "\n",
        "Use the input data to generate tailored advice for the candidate.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "z4iu9Ll7S4Re"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_zero_round = json.dumps({\n",
        "    \"original_resume\": paired_data[0][\"Resume\"],\n",
        "    \"Job_description\": paired_data[0][\"JobDescription\"],\n",
        "    \"Success_candidate_profile\": top_profiles\n",
        "})"
      ],
      "metadata": {
        "id": "x6XjbnEgcBA6"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "def get_response(persona, data):\n",
        "    user_prompt = \"Here are the json files of the job description and the corresponding successful candidate profile:\\n\" + data\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": persona}]},\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": user_prompt\n",
        "                    }\n",
        "                ]\n",
        "            },\n",
        "        ],\n",
        "        max_tokens=1000\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "ZNz3KnLTShb4"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "general_advice = get_response(InstructionGeneratorprompt, data_zero_round)"
      ],
      "metadata": {
        "id": "eB7WiuSGWWpO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_get_response(data, profiles, persona, client):\n",
        "    \"\"\"\n",
        "    Combines the generation of data_zero_round JSON and sends it to the OpenAI API.\n",
        "\n",
        "    Args:\n",
        "        paired_data (list): A list containing paired resume and job description data.\n",
        "        top_profiles (list): A list of the top successful candidate profiles.\n",
        "        persona (str): The persona or instruction to pass as the system prompt.\n",
        "        client (object): The OpenAI API client instance.\n",
        "\n",
        "    Returns:\n",
        "        str: The response content from the OpenAI API.\n",
        "    \"\"\"\n",
        "    # Generate the JSON payload for the first round\n",
        "    data_zero_round = json.dumps({\n",
        "        \"original_resume\": data[\"Resume\"],\n",
        "        \"Job_description\": data[\"JobDescription\"],\n",
        "        \"Success_candidate_profile\": profiles\n",
        "    })\n",
        "\n",
        "    # Create the prompt for the API\n",
        "    user_prompt = \"Here are the json files of the job description and the corresponding successful candidate profile:\\n\" + data_zero_round\n",
        "\n",
        "    # Send the request to the OpenAI API\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": persona},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "        max_tokens=1000\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "3Uz8hOazM-dx"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "general_advices = []\n",
        "\n",
        "for i in range(9):\n",
        "    general_advice = process_and_get_response(paired_data[i], top_profiles[i], InstructionGeneratorPrompt, client)\n",
        "    general_advices.append(general_advice)"
      ],
      "metadata": {
        "id": "NreGu98tOWci"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(9):\n",
        "    general_advices[0] = general_advices[0].strip(\"```json\").strip(\"```\").strip()"
      ],
      "metadata": {
        "id": "QMVZDqILUCG_"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZyI-iTsSWFiZ"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = f\"\"\"I am a highly experienced career advisor and resume writing expert with 15 years of specialized experience.\n",
        "\n",
        "    Primary role: Craft exceptional resumes and cover letters tailored to specific job descriptions, optimized for both ATS systems and human readers.\n",
        "\n",
        "    # Instructions for creating optimized resumes and cover letters\n",
        "    1. Analyze job descriptions:\n",
        "       - Extract key requirements and keywords\n",
        "       - Note: Adapt analysis based on specific industry and role\n",
        "\n",
        "    2. Create compelling resumes:\n",
        "       - Highlight quantifiable achievements (e.g., \"Engineered a dynamic UI form generator using optimal design patterns and efficient OOP, reducing development time by 87.5%\")\n",
        "       - Tailor content to specific job and company\n",
        "       - Emphasize candidate's unique value proposition\n",
        "\n",
        "    3. Craft persuasive cover letters:\n",
        "       - Align content with targeted positions\n",
        "       - Balance professional tone with candidate's personality\n",
        "       - Use a strong opening statement, e.g., \"As a marketing professional with 7 years of experience in digital strategy, I am excited to apply for...\"\n",
        "       - Identify and emphasize soft skills valued in the target role/industry. Provide specific examples demonstrating these skills\n",
        "\n",
        "    4. Optimize for Applicant Tracking Systems (ATS):\n",
        "       - Use industry-specific keywords strategically throughout documents\n",
        "       - Ensure content passes ATS scans while engaging human readers\n",
        "\n",
        "    5. Provide industry-specific guidance:\n",
        "       - Incorporate current hiring trends\n",
        "       - Prioritize relevant information (apply \"6-second rule\" for quick scanning)\n",
        "       - Use clear, consistent formatting\n",
        "\n",
        "    6. Apply best practices:\n",
        "       - Quantify achievements where possible\n",
        "       - Use specific, impactful statements instead of generic ones\n",
        "       - Update content based on latest industry standards\n",
        "       - Use active voice and strong action verbs\n",
        "\n",
        "    Note: Adapt these guidelines to each user's specific request, industry, and experience level.\n",
        "\n",
        "    Goal: Create documents that not only pass ATS screenings but also compellingly demonstrate how the user can add immediate value to the prospective employer.\n",
        "\n",
        "    Return your output strictly in the following JSON format:\n",
        "    {{\n",
        "        \"Revised Resumes\": [\n",
        "            {{\n",
        "                \"ResumeIndex\": 0,\n",
        "                \"ResumeJSON\": {{\n",
        "                    \"Education\": [\n",
        "                        {{\n",
        "                            \"Degree\": \"\",\n",
        "                            \"Institution\": \"\",\n",
        "                            \"CGPA\": \"\"\n",
        "                        }}\n",
        "                    ],\n",
        "                    \"Experience\": [\n",
        "                        {{\n",
        "                            \"Role\": \"\",\n",
        "                            \"Company\": \"\",\n",
        "                            \"Description\": \"\"\n",
        "                        }}\n",
        "                    ],\n",
        "                    \"Skills\": [\n",
        "                        \"\"\n",
        "                    ],\n",
        "                    \"Projects\": [\n",
        "                        {{\n",
        "                            \"Project\": \"\",\n",
        "                            \"Describe\": \"\"\n",
        "                        }}\n",
        "                    ]\n",
        "                }}\n",
        "            }}\n",
        "        ],\n",
        "        \"Career Development Advise\": [\n",
        "            {{\n",
        "                \"Skill\": \"\",\n",
        "                \"Advice\": [\n",
        "                    \"\"\n",
        "                ]\n",
        "            }}\n",
        "        ]\n",
        "    }}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "OJNJRYK3s5b8"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_llm_1(system_content, user_content):\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_content},\n",
        "                {\"role\": \"user\", \"content\": user_content}\n",
        "            ]\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "hTK76ZnMV_iJ"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_llm_output(output):\n",
        "    \"\"\"\n",
        "    Processes the LLM output and extracts the revised resume and career development advice.\n",
        "\n",
        "    Args:\n",
        "        optimized_output (str): The output string from the LLM, assumed to be JSON-formatted.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing:\n",
        "            - \"revised_resume\": The revised resume (if available).\n",
        "            - \"career_development_advice\": The career development advice (if available).\n",
        "            - \"error\": Error message if the output is invalid or missing.\n",
        "    \"\"\"\n",
        "    output = output.strip(\"```json\").strip(\"```\").strip()\n",
        "    if output:\n",
        "        try:\n",
        "            # Parse the JSON output from the LLM\n",
        "            first_round_results = json.loads(output)\n",
        "\n",
        "            # Extract individual parts\n",
        "            revised_resume = first_round_results.get(\"Revised Resumes\")\n",
        "            career_development_advice = first_round_results.get(\"Career Development Advise\")\n",
        "\n",
        "            return {\n",
        "                \"revised_resume\": revised_resume,\n",
        "                \"career_development_advice\": career_development_advice,\n",
        "                \"error\": None\n",
        "            }\n",
        "        except json.JSONDecodeError as e:\n",
        "            return {\n",
        "                \"revised_resume\": None,\n",
        "                \"career_development_advice\": None,\n",
        "                \"error\": f\"Failed to parse JSON: {str(e)}\"\n",
        "            }\n",
        "    else:\n",
        "        return {\n",
        "            \"revised_resume\": None,\n",
        "            \"career_development_advice\": None,\n",
        "            \"error\": \"Failed to get valid response from the LLM\"\n",
        "        }"
      ],
      "metadata": {
        "id": "aNLuMUSuVRNk"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_first_round = json.dumps({\n",
        "#     \"original_resume\": paired_data[0][\"Resume\"],\n",
        "#     \"Job_description\": paired_data[0][\"JobDescription\"],\n",
        "#     \"Advise\": general_advice\n",
        "# })\n",
        "# instruction_first_round = \" Given the inputs be two json file, original resumes, job description and incorporate advice from successful candidate. give output revised resume and career development advice.\"\n",
        "# first_round_systemprompt = system_prompt + \"\\n\" + instruction_first_round\n",
        "# optimized_output = test_llm_1(first_round_systemprompt, data_first_round)\n",
        "# optimized_output = process_llm_output(optimized_output)"
      ],
      "metadata": {
        "id": "kBT-4Sq-tqnl"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction_first_round = \" Given the inputs be two json file, original resumes, job description and incorporate advice from successful candidate. give output revised resume and career development advice.\"\n",
        "first_round_systemprompt = system_prompt + \"\\n\" + instruction_first_round\n",
        "optimized_output_all = []\n",
        "for i in range(9):\n",
        "    data_first_round = json.dumps({\n",
        "        \"original_resume\": paired_data[i][\"Resume\"],\n",
        "        \"Job_description\": paired_data[i][\"JobDescription\"],\n",
        "        \"Advise\": general_advices[i]\n",
        "    })\n",
        "    optimized_output = test_llm_1(first_round_systemprompt, data_first_round)\n",
        "    optimized_output = process_llm_output(optimized_output)\n",
        "    optimized_output_all.append(optimized_output)"
      ],
      "metadata": {
        "id": "Y_xxIuvbWlix"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_output_all[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70aZ7fFza5u6",
        "outputId": "6c7ad352-9ae5-4894-8bb2-dcb43288fd88"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'revised_resume': [{'ResumeIndex': 0,\n",
              "   'ResumeJSON': {'Education': [{'Degree': 'Honors Bachelor of Science in Physics, Minor in Computing',\n",
              "      'Institution': 'University of Waterloo',\n",
              "      'CGPA': ''}],\n",
              "    'Experience': [{'Role': 'Data Science Assurance Associate',\n",
              "      'Company': 'Ernst & Young LLP',\n",
              "      'Description': 'Led development of an automated review platform implementing predictive coding and topic modeling, reducing review costs and time. Conducted R&D on classification models and predictive analysis for fraud detection. Collaborated with cross-functional teams for innovative solutions using Python, scikit-learn, tfidf, word2vec, Tableau, and Elasticsearch. Engaged in development of a fraud analytic platform, optimizing ERP systems using HTML, JavaScript, SQL Server, and D3.js.'}],\n",
              "    'Skills': ['Python',\n",
              "     'SQL',\n",
              "     'Java',\n",
              "     'JavaScript',\n",
              "     'Machine Learning: Regression, SVM, Nave Bayes, KNN, Random Forest, etc.',\n",
              "     'Google Cloud Platform: BigQuery, BigTable, Vertex AI',\n",
              "     'Orchestration Tools: Apache Airflow',\n",
              "     'Containerization: Docker, Kubernetes',\n",
              "     'CI/CD Practices',\n",
              "     'Data Visualization: Tableau, matplotlib, D3.js',\n",
              "     'Deep Learning Concepts'],\n",
              "    'Projects': [{'Project': 'Fraud Detection Platform',\n",
              "      'Describe': 'Developed a platform leveraging advanced analytics for fraud detection in ERP systems, enhancing data processing efficiency.'},\n",
              "     {'Project': 'Chatbot Development',\n",
              "      'Describe': 'Designed a chatbot for customer queries using NLP and topic modeling, improving user query handling efficiency.'}]}}],\n",
              " 'career_development_advice': [{'Skill': 'Google Cloud Platform',\n",
              "   'Advice': ['Enrol in Google Cloud certifications to solidify understanding and demonstrate proficiency.',\n",
              "    'Practice by developing personal projects using GCP services such as BigTable and Vertex AI, and document key learnings.']},\n",
              "  {'Skill': 'Orchestration & Automation',\n",
              "   'Advice': ['Take courses on orchestration tools like Apache Airflow to enhance pipeline management skills.',\n",
              "    'Contribute to open-source projects or online communities related to Airflow or similar technologies.']},\n",
              "  {'Skill': 'Containerization & CI/CD Practices',\n",
              "   'Advice': ['Gain practical experience with Docker and Kubernetes through tutorials and personal projects.',\n",
              "    'Implement CI/CD pipelines in a mock project to understand the end-to-end process of integration and deployment.']}],\n",
              " 'error': None}"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM2"
      ],
      "metadata": {
        "id": "aMURIRXYynbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instructionSecond = (\n",
        "    \"Act as a career advisor reviewing three key documents: the original resume, the newly generated resume, \"\n",
        "    \"and the list of skills needing improvement. Your role is to carefully compare \"\n",
        "    \"and analyze these documents to identify enhancements and discrepancies. Approach your analysis with \"\n",
        "    \"a thoughtful and constructive perspective. Guidelines: Begin by comparing the original and revised \"\n",
        "    \"resumes. List all differences in the section skills, and then determine whether each new skill in the revised resume \"\n",
        "    \"aligns with the job description and is justifiably added. Unreasonable new skills are those that cannot be inferred \"\n",
        "    \"from the candidate's past education or experience. For example, we can reasonably assume that a computer science \"\n",
        "    \"student knows Java; however, if the resume claims expertise in a specialized tool like Kubernetes without relevant \"\n",
        "    \"professional experience or specific training, it should be questioned. For the skills analysis, \"\n",
        "    \"compare the skills listed in the original resume against those in the skills needing improvement list. \"\n",
        "    \"Identify which skills have already been developed through past experiences, even if not explicitly mentioned, \"\n",
        "    \"like soft skills or technical abilities such as Git. After your initial review, seek feedback \"\n",
        "    \"to refine your approach and enhance the candidates portrayal in their resume. The output should consist of two parts: \"\n",
        "    \"1. Suggestion for the new skill added in the revised resume: Provide specific foundation if the skill was added based on past experience or project. Remove the skill do not have solid foundation. \"\n",
        "    \"2. Suggestions for the skill improvement set: Identify skills listed as needing improvement to find if they have already been built during past experience or project.\"\n",
        ")"
      ],
      "metadata": {
        "id": "tkPH5qlQyEdy"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def test_llm_2(system_content, user_content):\n",
        "#     try:\n",
        "#         response = client.chat.completions.create(\n",
        "#             model=\"gpt-4o\",\n",
        "#             messages=[\n",
        "#                 {\"role\": \"system\", \"content\": system_content},\n",
        "#                 {\"role\": \"user\", \"content\": user_content}\n",
        "#             ]\n",
        "#         )\n",
        "#         return response.choices[0].message.content\n",
        "#     except Exception as e:\n",
        "#         print(f\"An error occurred: {e}\")\n",
        "#         return None"
      ],
      "metadata": {
        "id": "9MC61x4pzGWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_LLM2 = json.dumps({\n",
        "    \"original_resume\": paired_data[0][\"Resume\"],\n",
        "    \"revised_resume\": optimized_output[\"revised_resume\"],\n",
        "    \"skills_to_improve\": optimized_output[\"career_development_advice\"]\n",
        "})\n",
        "AIsuggestion = test_llm_1(instructionSecond, data_LLM2)"
      ],
      "metadata": {
        "id": "FThGijwKzghb"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_output_all[8][\"career_development_advice\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iH4cDQncEga",
        "outputId": "74963e87-0638-497c-a7c6-c524c5c18d7e"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Skill': 'Machine Learning Frameworks',\n",
              "  'Advice': ['Gain practical experience with TensorFlow and scikit-learn to enhance proficiency. Obtaining certifications in these technologies is recommended.']},\n",
              " {'Skill': 'Cloud Computing',\n",
              "  'Advice': ['Acquire practical experience with AWS or Azure platforms. Consider certifications to bolster your credentials.']},\n",
              " {'Skill': 'Leadership and Project Direction',\n",
              "  'Advice': ['Develop leadership and team guiding abilities through structured communities or workshops. Consider volunteer roles or consultative projects to enhance experience in guiding ML projects.']}]"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AIsuggestion_all = []\n",
        "for i in range(9):\n",
        "    data_LLM2 = json.dumps({\n",
        "    \"original_resume\": paired_data[i][\"Resume\"],\n",
        "    \"revised_resume\": optimized_output_all[i][\"revised_resume\"],\n",
        "    \"skills_to_improve\": optimized_output_all[i][\"career_development_advice\"]\n",
        "    })\n",
        "    AIsuggestion = test_llm_1(instructionSecond, data_LLM2)\n",
        "    AIsuggestion_all.append(AIsuggestion)"
      ],
      "metadata": {
        "id": "gIRxRWE0bH66"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AIsuggestion_all[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "n_MmES6Pd2_j",
        "outputId": "d98ad53c-3867-4545-d2ec-8660f2aa3e49"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"### Initial Review and Suggestions\\n\\n#### Differences in Skills Section:\\n\\n1. **New Skills Added in the Revised Resume**:\\n   - **Google Cloud Platform: BigQuery, BigTable, Vertex AI**\\n   - **Orchestration Tools: Apache Airflow**\\n   - **Containerization: Docker, Kubernetes**\\n   - **CI/CD Practices**\\n\\n2. **Justification and Suggestions for Each New Skill**:\\n\\n   - **Google Cloud Platform: BigQuery, BigTable, Vertex AI**\\n     - **Suggestion**: This skill is not explicitly mentioned or supported by the candidate's experience in the original resume. Without specific past projects or roles involving GCP, it might be premature to claim proficiency. Enhancing this skill through relevant courses and projects would provide a solid basis for inclusion.\\n   \\n   - **Orchestration Tools: Apache Airflow**\\n     - **Suggestion**: This is not present in the original resume. If there is no evidence of practical use in past roles or projects, it should be removed. Course completion or open-source contributions involving Airflow could substantiate its addition.\\n\\n   - **Containerization: Docker, Kubernetes**\\n     - **Docker**: Previously listed in the original resume, indicating a solid foundation.\\n     - **Kubernetes**: New addition with no clear supporting experience. This skill should be removed unless the candidate has undertaken specific training or relevant projects.\\n\\n   - **CI/CD Practices**\\n     - **Suggestion**: This is a broad skill and could potentially have been developed through work involving Git and other tools; however, without explicit mention or related experiences, further development or evidence is needed before inclusion.\\n\\n#### Skills Needing Improvement:\\n\\n1. **Analysis against Original Resume Experience**:\\n\\n   - **Google Cloud Platform**: No relevant experience in the original resume, supporting the improvement advice for gaining this knowledge.\\n   - **Orchestration & Automation (Apache Airflow)**: Not evidenced in the original resume; advice aligns well with the lack of listed experience.\\n   - **Containerization**: Docker is included and suggests some familiarity. Kubernetes lacks any foundation; hence, mentioned advice is fitting.\\n   - **CI/CD Practices**: Potential basic understanding of deployment processes might exist through version control experience. However, deeper hands-on projects would enhance proficiency.\\n\\n### Feedback and Suggestions for Resume Refinement:\\n\\n1. **Foundation for New Skills**:\\n   - Make sure skills added in the revised resume are founded on past experiences or projects. For GCP and Airflow, note specific projects undertaken or professional development activities.\\n\\n2. **Remove Unfounded Skills**:\\n   - For K8s and CI/CD which lack explicit mentions or foundations in the candidates previous work, focus on developing these skills first in suitable environments before including them on the resume.\\n\\n3. **Professional Development Projects**:\\n   - Encourage the undertaking of professional-quality projects, especially for cloud platforms and CI/CD, to solidify the practical understanding showcased in the resume.\\n\\n4. **Continuity in Narrating Skills**:\\n   - Describe how each skill has been applied in past roles or projects to maintain a coherent narrative of professional development. For instance, link tool proficiency with specific project outcomes.\\n\\nThis structured approach in enhancing skills and verifying strict relevance to the individual's experiences will refine the candidate's resume, making it more credible and job-market ready.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AI Career Advisor for Resume & Advise Generator Incorporate Suggestion"
      ],
      "metadata": {
        "id": "qfHJvm040xMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ImproveInstruction = f\"\"\"\n",
        "As a career advisor, you are tasked with critically analyzing the provided resume against the job description, based on provided input suggestion. Implement these specific, mandatory changes:\n",
        "- **Skill Verification**: Thoroughly review each skill listed on the revised resume. Remove any skills that lack a verifiable foundation from the applicants documented education, direct experiences, or completed projects.\n",
        "- **Quantify Achievements**: Ensure all listed achievements are backed by quantifiable evidence or specific outcomes. Where necessary, provide exact directives on which figures or data should be included to support these claims.\n",
        "- **Active Language Enhancement**: Analyze the use of verbs in the experience section. Replace any passive or vague verbs with strong, definitive action verbs that clearly showcase the candidates direct contributions and impact.\n",
        "- **Alignment with Job Requirements**: Confirm that every section of the resume not only meets ATS compliance but is also finely tuned to resonate with human recruiters, ensuring precise alignment with the job description.\n",
        "The goal is to transform the resume into a factual, persuasive narrative that effectively conveys the candidate's qualifications and preparedness for the role.\n",
        "\n",
        "Inputs to review include:\n",
        "- 'original_resume'\n",
        "- 'job_description': processed_job_descriptions\n",
        "- 'advice from successful candidate'\n",
        "- 'revised_resume'\n",
        "- 'career_development_advice'\n",
        "- 'suggestion'\n",
        "\n",
        "Your output should include an updated resume and career development advice that reflects these enhancements, formatted as specified. All newly generated numerical achievements in the\n",
        "revised resume that do not exist in the original resume must be represented using \"XX\" to avoid potential hallucination or inaccuracies.\n",
        "\"\"\"\n",
        "Highest_standard_rubrics = \"\"\"\n",
        "1. **Alignment with Job Description**: The resume is exceptionally tailored to the specific job description, incorporating all key requirements and industry-specific keywords.\n",
        "\n",
        "2. **Highlighting Relevant Skills and Experiences**: Expertly showcases the candidate's most relevant skills, experiences, and quantifiable achievements aligned with the job requirements.\n",
        "\n",
        "3. **Clarity and Readability**: The resume is exceptionally clear and easy to read, with information presented logically and succinctly.\n",
        "\n",
        "4. **Organization and Structure**: Highly organized with intuitive headings, seamless flow, and effective use of bullet points for readability.\n",
        "\n",
        "5. **Grammar, Spelling, and Punctuation**: Error-free, with flawless grammar, spelling, and punctuation throughout.\n",
        "\n",
        "6. **Professional Tone and Language**: Consistently uses a professional and industry-appropriate tone and language.\n",
        "\n",
        "7. **Visual Presentation**: Visually polished, with consistent formatting, appropriate fonts, balanced spacing, and an overall professional aesthetic.\n",
        "\n",
        "8. **Relevance and Conciseness**: Focuses entirely on relevant and impactful content, effectively removing any unnecessary or outdated information.\n",
        "\n",
        "9. **Accuracy and Authenticity**: Ensures all information is truthful and authentic, with no fabrications or exaggerations.\n",
        "\n",
        "10. **Preservation of Original Information**: Retains all essential details from the original resume while significantly enhancing presentation and alignment.\n",
        "\"\"\"\n",
        "combined_system_prompt = system_prompt + \"\\n\" + ImproveInstruction + \"\\n\" + Highest_standard_rubrics"
      ],
      "metadata": {
        "id": "224vkr7F0m5Y"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_second_round = json.dumps({\n",
        "    \"original_resume\": paired_data[0][\"Resume\"],\n",
        "    \"Job_description\": paired_data[0][\"JobDescription\"],\n",
        "    \"Advise\": general_advice,\n",
        "    \"revised_resume\": optimized_output['revised_resume'],\n",
        "    \"career_development_advice\": optimized_output['career_development_advice'],\n",
        "    \"suggestion\" :  AIsuggestion   # output from suggestion\n",
        "})"
      ],
      "metadata": {
        "id": "NqQNMVo41Buv"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "general_advices[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "Sp1L7viEfX_G",
        "outputId": "83fb83d9-97fa-4173-edce-5172ec512124"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'**Summary of Key Findings**:\\n1. **Gaps in Original Resume**:\\n   - Lack of specific cloud platform experience, particularly Google Cloud Platform (GCP) services such as Vertex AI, BigTable, BigQuery, and Cloud Composer.\\n   - Limited focus on automating ML pipelines and orchestration, which are key aspects of the target job.\\n   - Absence of direct experience with DevOps practices or orchestration tools like Apache Airflow (Composer).\\n   - Little emphasis on containerization (Docker, Kubernetes) and CI/CD pipelines which are highly valued in the target role.\\n   - The professional summary does not effectively capture a customer-centric mindset or collaborative experience with cross-functional teams.\\n\\n2. **Alignments**:\\n   - Strong proficiency in Python and SQL, aligning well with the target job requirement.\\n   - Experience in feature engineering, data processing, and analytics, which are essential for the target role.\\n   - Exposure to prominent machine learning methodologies and tools, such as scikit-learn and Tableau.\\n\\n**Step-by-Step Resume Revision Instructions**:\\n\\n1. **Professional Summary**:\\n   - Rephrase to include more industry-specific terminology such as cloud infrastructure, automation, and collaboration.\\n   - Example: Experienced Data Scientist skilled in deploying automated ML pipelines and enhancing platform scalability using cloud technologies. Proficient in Python and SQL, with a strong customer-centric approach and proven ability to collaborate with cross-functional teams.\\n\\n2. **Skills**:\\n   - Add GCP-related skills (BigQuery, BigTable, Cloud Composer) if applicable; if not, focus on gaining experience or learning these tools.\\n   - Refine skills section to highlight CI/CD tools, Apache Airflow, Docker, and Kubernetes.\\n\\n3. **Work Experience**:\\n   - For each position, emphasize achievements in automation and scalability.\\n   - Quantify results where possible (e.g., Reduced processing time by X%).\\n   - Add a bullet point discussing collaboration with cross-functional teams or any relevant experience with GCP or similar platforms.\\n\\n4. **Education and Certifications**:\\n   - Suggest adding a section for Certifications if none exist. Recommend pursuing relevant certifications like Google Cloud Certified  Professional Cloud Architect or a course on Apache Airflow.\\n   - Highlight any coursework or projects that involve cloud technologies or DevOps practices.\\n\\n**Career Development Suggestions**:\\n\\n1. **Skill Gap Analysis**:\\n   - **Critical Areas for Improvement**:\\n     - Gain proficiency in Google Cloud Platform services and orchestration tools.\\n     - Enhance automation skills, focusing on developing CI/CD pipelines and containerization.\\n\\n2. **Actionable Recommendations**:\\n   - **Certifications**: Consider enrolling in Google Cloud certifications (e.g., Professional Data Engineer).\\n   - **Courses**: Utilize platforms like Coursera, edX, or LinkedIn Learning for courses on GCP, Kubernetes, Docker, and Apache Airflow.\\n   - **Networking**: Attend cloud engineering meetups, webinars, or conferences to gain deeper insights and industry connections.\\n\\n3. **Experience Building**:\\n   - Apply for internships or part-time work focusing on cloud infrastructure projects.\\n   - Volunteer or freelance to gain practical experience with GCP or similar technologies.\\n   - Participate in hackathons or competitions focused on cloud solutions or machine learning challenges.\\n\\n4. **Targeted Achievements**:\\n   - Set goals to implement a personal project using GCP services, documenting processes and results to add to the resume.\\n   - Aim to complete a relevant certification within the next 3 months to bolster cloud skills.\\n\\n5. **Resume Enhancement Activities**:\\n   - Write articles or blogs on recent projects or learnings in cloud computing or machine learning.\\n   - Gain LinkedIn recommendations from peers or mentors in related roles to enhance credibility.\\n   - Once new skills are acquired, update your resume with specific, quantified achievements (e.g., Implemented a GCP-based solution reducing operational costs by XX%). \\n\\nBy addressing these gaps and building new experiences, the candidate can significantly improve their alignment with the target role and stand out in future applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FinalResponse_all = []\n",
        "for i in range(9):\n",
        "    data_second_round = json.dumps({\n",
        "        \"original_resume\": paired_data[i][\"Resume\"],\n",
        "        \"Job_description\": paired_data[i][\"JobDescription\"],\n",
        "        \"Advise\": general_advices[i],\n",
        "        \"revised_resume\": optimized_output_all[i]['revised_resume'],\n",
        "        \"career_development_advice\": optimized_output_all[i]['career_development_advice'],\n",
        "        \"suggestion\" :  AIsuggestion[i]   # output from suggestion\n",
        "    })\n",
        "    Final = test_llm_1(combined_system_prompt, data_second_round)\n",
        "    Final = process_llm_output(Final)\n",
        "    FinalResponse_all.append(Final)"
      ],
      "metadata": {
        "id": "7B9_nkeyd9NX"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FinalRevisedResume = []\n",
        "for i in range(9):\n",
        "    FinalResponse_all[i]['revised_resume']\n",
        "    FinalRevisedResume.append(FinalResponse_all[i]['revised_resume'])"
      ],
      "metadata": {
        "id": "l8OcMZG0i_nD"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FinalAdvices = []\n",
        "for i in range(9):\n",
        "    FinalResponse_all[i]['career_development_advice']\n",
        "    FinalAdvices.append(FinalResponse_all[i]['career_development_advice'])"
      ],
      "metadata": {
        "id": "pjO0AqBbnhC7"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FinalResponse = test_llm_1(combined_system_prompt, data_second_round)\n",
        "# FinalResponse = process_llm_output(FinalResponse)\n",
        "# FinalResponse['revised_resume']"
      ],
      "metadata": {
        "id": "b3wB344T8v1m"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fpdf import FPDF\n",
        "from fpdf import FPDF\n",
        "\n",
        "class ResumePDF(FPDF):\n",
        "    def header(self):\n",
        "        self.set_font(\"Arial\", \"B\", 12)\n",
        "        self.cell(0, 10, \"Revised Resume\", align=\"C\", ln=True)\n",
        "        self.ln(5)\n",
        "\n",
        "    def section_title(self, title):\n",
        "        self.set_font(\"Arial\", \"B\", 10)\n",
        "        self.cell(0, 8, title, ln=True, align=\"L\")\n",
        "        self.ln(3)\n",
        "\n",
        "    def add_text(self, text):\n",
        "        self.set_font(\"Arial\", size=9)\n",
        "        self.multi_cell(0, 5, text, align=\"L\")\n",
        "\n",
        "    def add_bullet_text(self, text):\n",
        "        self.set_font(\"Arial\", size=9)\n",
        "        self.cell(5, 5, \"-\")\n",
        "        self.multi_cell(0, 5, text, align=\"L\")\n",
        "\n",
        "def generate_resume_pdf(revised_resume, output_filename=\"Revised_Resume.pdf\"):\n",
        "    pdf = ResumePDF()\n",
        "    pdf.set_auto_page_break(auto=False)  # Disable auto page break to fit content manually\n",
        "    pdf.add_page()\n",
        "\n",
        "    # Adjust Margins\n",
        "    pdf.set_left_margin(10)\n",
        "    pdf.set_right_margin(10)\n",
        "\n",
        "    # Add Resume Sections\n",
        "    for resume in revised_resume:\n",
        "        resume_data = resume[\"ResumeJSON\"]\n",
        "\n",
        "        # Key Competencies & Skills Section\n",
        "        pdf.section_title(\"KEY COMPETENCIES & SKILLS\")\n",
        "        skills = resume_data.get(\"Skills\", [])\n",
        "        for skill in skills:\n",
        "            pdf.add_bullet_text(skill)\n",
        "        pdf.ln(2)\n",
        "\n",
        "        # Education Section\n",
        "        pdf.section_title(\"EDUCATION\")\n",
        "        for edu in resume_data.get(\"Education\", []):\n",
        "            edu_text = f\"{edu['Degree']}, {edu['Institution']}\"\n",
        "            if edu.get(\"CGPA\"):\n",
        "                edu_text += f\" | CGPA: {edu['CGPA']}\"\n",
        "            pdf.add_text(edu_text)\n",
        "        pdf.ln(2)\n",
        "\n",
        "        # Professional Experience Section\n",
        "        pdf.section_title(\"PROFESSIONAL EXPERIENCE\")\n",
        "        for exp in resume_data.get(\"Experience\", []):\n",
        "            exp_text = f\"{exp['Role']} - {exp['Company']}\"\n",
        "            pdf.add_text(exp_text)\n",
        "            pdf.add_text(f\"{exp['Description'][:200]}...\")  # Limit to 200 characters for brevity\n",
        "            pdf.ln(2)\n",
        "\n",
        "        # Relevant Projects Section\n",
        "        pdf.section_title(\"RELEVANT PROJECT\")\n",
        "        for project in resume_data.get(\"Projects\", []):\n",
        "            pdf.add_text(f\"Project: {project['Project']}\")\n",
        "            pdf.add_text(f\"{project['Describe'][:200]}...\")  # Limit to 200 characters for brevity\n",
        "            pdf.ln(2)\n",
        "\n",
        "    # Output PDF\n",
        "    pdf.output(output_filename)\n",
        "    print(f\"PDF successfully saved as {output_filename}\")\n",
        "\n",
        "generate_resume_pdf(FinalResponse['revised_resume'], output_filename=\"Revised_Resume.pdf\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wNIUric-sll",
        "outputId": "3f91ed9c-5208-4fbc-e4fd-bba6f67a7e21"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF successfully saved as Revised_Resume.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResumePDF(FPDF):\n",
        "    def header(self):\n",
        "        self.set_font(\"Arial\", \"B\", 14)\n",
        "        self.cell(0, 10, \"Revised Resume\", align=\"C\", ln=True)\n",
        "        self.ln(10)\n",
        "\n",
        "    def section_title(self, title):\n",
        "        self.set_font(\"Arial\", \"B\", 12)\n",
        "        self.cell(0, 10, title, ln=True, align=\"L\")\n",
        "        self.ln(5)\n",
        "\n",
        "    def add_text(self, text, indent=0):\n",
        "        self.set_font(\"Arial\", size=10)\n",
        "        self.multi_cell(0, 8, text, align=\"L\")\n",
        "\n",
        "    def add_bullet_text(self, text):\n",
        "        self.set_font(\"Arial\", size=10)\n",
        "        self.cell(5, 8, \"-\")\n",
        "        self.multi_cell(0, 8, text, align=\"L\")\n",
        "\n",
        "def generate_resume_pdf(revised_resume, output_filename=\"Revised_Resume.pdf\"):\n",
        "    pdf = ResumePDF()\n",
        "    pdf.add_page()\n",
        "\n",
        "    # Add Resume Sections\n",
        "    for resume in revised_resume:\n",
        "        resume_data = resume[\"ResumeJSON\"]\n",
        "\n",
        "        # Key Competencies & Skills Section\n",
        "        pdf.section_title(\"KEY COMPETENCIES & SKILLS\")\n",
        "        skills = resume_data.get(\"Skills\", [])\n",
        "        for skill in skills:\n",
        "            pdf.add_bullet_text(skill)\n",
        "        pdf.ln(5)\n",
        "\n",
        "        # Education Section\n",
        "        pdf.section_title(\"EDUCATION\")\n",
        "        for edu in resume_data.get(\"Education\", []):\n",
        "            edu_text = f\"{edu['Degree']}, {edu['Institution']}\"\n",
        "            if edu.get(\"CGPA\"):\n",
        "                edu_text += f\" | CGPA: {edu['CGPA']}\"\n",
        "            pdf.add_text(edu_text)\n",
        "        pdf.ln(5)\n",
        "\n",
        "        # Professional Experience Section\n",
        "        pdf.section_title(\"PROFESSIONAL EXPERIENCE\")\n",
        "        for exp in resume_data.get(\"Experience\", []):\n",
        "            exp_text = f\"{exp['Role']} - {exp['Company']}\"\n",
        "            pdf.add_text(exp_text)\n",
        "            pdf.add_text(f\"  {exp['Description']}\")\n",
        "            pdf.ln(3)\n",
        "\n",
        "        # Relevant Projects Section\n",
        "        pdf.section_title(\"RELEVANT PROJECT\")\n",
        "        for project in resume_data.get(\"Projects\", []):\n",
        "            pdf.add_text(f\"Project: {project['Project']}\")\n",
        "            pdf.add_text(f\"  {project['Describe']}\")\n",
        "            pdf.ln(3)\n",
        "\n",
        "    # Output PDF\n",
        "    pdf.output(output_filename)\n",
        "generate_resume_pdf(revised_resume, output_filename=\"Revised_Resume.pdf\")"
      ],
      "metadata": {
        "id": "j1KS1aGUBpX5"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"Revised_Resume.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "mDPQub9l-4Lb",
        "outputId": "7f90cac8-0901-414b-fd02-fb9cf22f5d2c"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1ad8744b-59ed-4cf1-a6cc-5cafbe0929ca\", \"Revised_Resume.pdf\", 2017)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RaM1nCAmf9k8"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"originalresume_jd.json\", \"w\") as json_file:\n",
        "    json.dump(paired_data[:9], json_file)\n",
        "print(\"JSON file saved as originalresume_jd.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9NdGsUqglgx",
        "outputId": "677b38c2-61c0-4534-fee4-0a95875f82af"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON file saved as originalresume_jd.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"originalresume_jd.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "DbQux6i-hlso",
        "outputId": "ef5e6597-2afa-4652-cfec-edd2e41b398c"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d1ed9f5f-0ec8-44fd-8281-c613ca0c959a\", \"originalresume_jd.json\", 41139)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"revised_resume_advice.json\", \"w\") as json_file:\n",
        "    json.dump(FinalRevisedResume, json_file)\n",
        "\n",
        "print(\"JSON file saved as data.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI5wHsqEhg4w",
        "outputId": "9627d475-4ce4-48c2-9b23-05535ab1556a"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON file saved as data.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"revised_resume_advice.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Wen4zRpThtPc",
        "outputId": "7236349d-442e-4b04-f17b-0feb1ab6fe3d"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ee4d53f3-f3cf-45c6-b51f-f54322182a70\", \"revised_resume_advice.json\", 12927)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"revised_advice.json\", \"w\") as json_file:\n",
        "    json.dump(FinalAdvices, json_file)\n",
        "print(\"JSON file saved as data.json\")\n",
        "files.download(\"revised_advice.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7Ei7gatMjUWD",
        "outputId": "23369206-54e5-454f-98c9-b107b72071e4"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON file saved as data.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_94378c13-9a4c-4ef4-bfd9-43600ff2a444\", \"revised_advice.json\", 9320)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}