[{"Resume": {"Education": "PGDM in Business Analytics\nInstitution: Great Lakes Institute of Management & Illinois Institute of Technology\nDuration: January 2017 \u2013 January 2018\nBachelor of Engineering in Electronics and Communication\nInstitution: New Horizon College of Engineering, Bengaluru (Visvesvaraya Technological University)\nGraduation Date: January 2013", "Experience": "Consultant \u2013 Deloitte USI\nProject 1: Historic Deals Analysis\n\nIndustry: Cross Industry\nDuration: 6 months\nDescription: Conducted end-to-end analysis of historic deals, providing insights for price optimization and process improvement.\nKey Responsibilities:\nExtracted data across geographies.\nBuilt Tableau reports and generated insights.\nTechnical Tools: R, Tableau\nProject 2: Handwriting Recognition\n\nIndustry: Finance\nDuration: 3 months\nDescription: Developed functionalities to convert handwritten images into digital text using LSTM models.\nKey Responsibilities:\nGathered data for English words.\nTrained LSTM models for sentence correction.\nTechnical Tools: Python\nProject 3: Financial Reporting Infrastructure (SWIFT)\n\nIndustry: Finance\nDuration: 8 months\nDescription: Developed analytics infrastructure for financial reporting and expense forecasting on SAP S/4.\nKey Responsibilities:\nDesigned and developed data models.\nBuilt ETL processes and validated reports.\nTechnical Tools: SAP HANA, Tableau, SAP AO\nProject 4: Clinical Healthcare System\n\nIndustry: Healthcare Analytics\nDuration: 2 months\nDescription: Developed analytics infrastructure with advanced analytics capabilities on top of Argus.\nKey Responsibilities:\nDesigned and developed data models.\nBuilt analytical models and validated reports.\nTechnical Tools: Data Modeling, SAP HANA, Tableau, NLP\nProject 5: Consumption-Based Planning (FMCG)\n\nIndustry: FMCG\nDuration: 8 months\nDescription: Implemented CRM and CBP modules for forecasting and performance improvements.\nKey Responsibilities:\nDesigned HANA models and developed data flow for forecasts.\nBuilt customer/sales/funds reporting views.\nTechnical Tools: SAP HANA, BOBJ, Time Series Forecasting\nInternal Initiative: Customer Segmentation & RFM Analysis\n\nIndustry: FMCG\nDuration: 3 months\nDescription: Set up a HANA-Python interface to segment customers and perform advanced analytics.\nKey Achievements:\nSegmented data using K-means and performed RFM analysis.\nDeveloped algorithms to categorize new customers.\nTechnical Tools: Python, HANA\nInternal Initiative: Telecom Invoice State Detection\n\nIndustry: Telecom\nDuration: 1 month\nDescription: Automated invoice state detection using decision trees, reducing manual effort by 60%.\nTechnical Tools: R, SAP PAL, SAP HANA\nSenior Developer \u2013 Accenture\nProject 1: In Process Analytics (IPA)\n\nIndustry: Cross Industry\nDuration: 19 months\nDescription: Developed database objects and data models to provide operational insights using SAP analytics tools.\nKey Responsibilities:\nDesigned SAP HANA models (Attribute, Analytic, Calculation views).\nDeveloped KPIs using complex SQL scripts.\nBuilt dashboards and implemented predictive analytics.\nTechnical Tools: R, SAP HANA, T-SQL\nProject 2: Testing Accelerator for SAP (ATAS)\n\nIndustry: Cross Industry\nDuration: 21 months\nDescription: Led development and deployment of the ATAS tool for database management and analytics.\nKey Responsibilities:\nDesigned database objects (tables, views, stored procedures).\nNormalized database tables and performed performance tuning.\nManaged production and quality database servers.\nTechnical Tools: SQL Server, C#, PL-SQL, T-SQL", "Skills": "Programming & Tools: Python, R, C#, SQL, Tableau, SAP HANA, SAP HANA SQL, SAP HANA PAL, SAP Lumira\nFrameworks & Platforms: Django, Flask\nAdvanced Analytics: Linear Programming, Data Modeling, NLP, Retail Analytics, SCM Analytics, Social Media Analytics\nSpecialized Knowledge: Time Series Forecasting, K-Means Clustering, RFM Analysis", "Projects": "Historic Deals Analysis, Handwriting Recognition, Financial Reporting Infrastructure (SWIFT), Clinical Healthcare System, Consumption-Based Planning (FMCG), Customer Segmentation & RFM Analysis, Telecom Invoice State Detection, In Process Analytics (IPA), Testing Accelerator for SAP (ATAS)"}, "JobDescription": {"Role": "Design, build, and maintain highly scalable, robust, and efficient cloud infrastructure using Google Cloud Platform (GCP) services, including Vertex AI, BigTable, BigQuery, and Cloud Composer. Develop automation and orchestration of ML pipelines, integrating data ingestion, feature engineering, training, and deployment processes. Collaborate with cross-functional teams to understand their needs and build solutions that improve platform usability, scalability, and the overall development experience. Optimize data processing pipelines and cloud resources to ensure low-latency, cost-effective operation. Implement monitoring, alerting, and failover strategies to ensure platform reliability. Stay updated with industry trends and best practices in cloud engineering, data engineering, and machine learning", "Qualification": "Customer-centric mindset: Passionate about delivering an exceptional experience for data scientists through a self-service platform, reducing friction in their workflows. Collaboration: Strong communication skills to work closely with cross-functional teams, including data scientists and engineers, to ensure platform features meet user needs and expectations. Problem-solving: Ability to identify and solve complex technical issues related to ML pipelines, cloud infrastructure, and scalability, ensuring an efficient and robust platform. Automation-first approach: Commitment to streamlining and automating processes for scalability and reliability, enabling data scientists to focus on experimentation and model development. Adaptability: Ability to quickly adjust to new technologies and evolving platform needs to keep the infrastructure cutting-edge and efficient. Ownership and initiative: Comfortable taking ownership of key platform components, driving innovation and improvements that benefit the platform\u2019s scalability and usability. Bachelor\u2019s or Master\u2019s degree in Computer Science, Engineering, or a related field. 2+ years of experience in software engineering with a focus on cloud infrastructure and/or data engineering. Hands-on experience with Google Cloud Platform services such as Vertex AI, BigTable, BigQuery, Cloud Composer, Cloud Storage, etc. Proficiency in one or more programming languages such as Python, Java, and SQL. Experience with orchestration tools such as Apache Airflow (Composer). Knowledge of CI/CD pipelines and DevOps tools for continuous integration and deployment. Familiarity with containerization and orchestration (Docker, Kubernetes). Strong problem-solving skills and attention to detail. Excellent communication skills and ability to work in a collaborative, fast-paced environment", "Title": "SDE"}}, {"Resume": {"Education": "PGDM in Business Analytics\nInstitution: Great Lakes Institute of Management & Illinois Institute of Technology\nDuration: January 2017 \u2013 January 2018\nBachelor of Engineering in Electronics and Communication\nInstitution: New Horizon College of Engineering, Bengaluru (Visvesvaraya Technological University)\nGraduation Date: January 2013", "Experience": "Consultant \u2013 Deloitte USI\nProject 1: Historic Deals Analysis\n\nIndustry: Cross Industry\nDuration: 6 months\nDescription: Conducted end-to-end analysis of historic deals, providing insights for price optimization and process improvement.\nKey Responsibilities:\nExtracted data across geographies.\nBuilt Tableau reports and generated insights.\nTechnical Tools: R, Tableau\nProject 2: Handwriting Recognition\n\nIndustry: Finance\nDuration: 3 months\nDescription: Developed functionalities to convert handwritten images into digital text using LSTM models.\nKey Responsibilities:\nGathered data for English words.\nTrained LSTM models for sentence correction.\nTechnical Tools: Python\nProject 3: Financial Reporting Infrastructure (SWIFT)\n\nIndustry: Finance\nDuration: 8 months\nDescription: Developed analytics infrastructure for financial reporting and expense forecasting on SAP S/4.\nKey Responsibilities:\nDesigned and developed data models.\nBuilt ETL processes and validated reports.\nTechnical Tools: SAP HANA, Tableau, SAP AO\nProject 4: Clinical Healthcare System\n\nIndustry: Healthcare Analytics\nDuration: 2 months\nDescription: Developed analytics infrastructure with advanced analytics capabilities on top of Argus.\nKey Responsibilities:\nDesigned and developed data models.\nBuilt analytical models and validated reports.\nTechnical Tools: Data Modeling, SAP HANA, Tableau, NLP\nProject 5: Consumption-Based Planning (FMCG)\n\nIndustry: FMCG\nDuration: 8 months\nDescription: Implemented CRM and CBP modules for forecasting and performance improvements.\nKey Responsibilities:\nDesigned HANA models and developed data flow for forecasts.\nBuilt customer/sales/funds reporting views.\nTechnical Tools: SAP HANA, BOBJ, Time Series Forecasting\nInternal Initiative: Customer Segmentation & RFM Analysis\n\nIndustry: FMCG\nDuration: 3 months\nDescription: Set up a HANA-Python interface to segment customers and perform advanced analytics.\nKey Achievements:\nSegmented data using K-means and performed RFM analysis.\nDeveloped algorithms to categorize new customers.\nTechnical Tools: Python, HANA\nInternal Initiative: Telecom Invoice State Detection\n\nIndustry: Telecom\nDuration: 1 month\nDescription: Automated invoice state detection using decision trees, reducing manual effort by 60%.\nTechnical Tools: R, SAP PAL, SAP HANA\nSenior Developer \u2013 Accenture\nProject 1: In Process Analytics (IPA)\n\nIndustry: Cross Industry\nDuration: 19 months\nDescription: Developed database objects and data models to provide operational insights using SAP analytics tools.\nKey Responsibilities:\nDesigned SAP HANA models (Attribute, Analytic, Calculation views).\nDeveloped KPIs using complex SQL scripts.\nBuilt dashboards and implemented predictive analytics.\nTechnical Tools: R, SAP HANA, T-SQL\nProject 2: Testing Accelerator for SAP (ATAS)\n\nIndustry: Cross Industry\nDuration: 21 months\nDescription: Led development and deployment of the ATAS tool for database management and analytics.\nKey Responsibilities:\nDesigned database objects (tables, views, stored procedures).\nNormalized database tables and performed performance tuning.\nManaged production and quality database servers.\nTechnical Tools: SQL Server, C#, PL-SQL, T-SQL", "Skills": "Programming & Tools: Python, R, C#, SQL, Tableau, SAP HANA, SAP HANA SQL, SAP HANA PAL, SAP Lumira\nFrameworks & Platforms: Django, Flask\nAdvanced Analytics: Linear Programming, Data Modeling, NLP, Retail Analytics, SCM Analytics, Social Media Analytics\nSpecialized Knowledge: Time Series Forecasting, K-Means Clustering, RFM Analysis", "Projects": "Historic Deals Analysis, Handwriting Recognition, Financial Reporting Infrastructure (SWIFT), Clinical Healthcare System, Consumption-Based Planning (FMCG), Customer Segmentation & RFM Analysis, Telecom Invoice State Detection, In Process Analytics (IPA), Testing Accelerator for SAP (ATAS)"}, "JobDescription": {"Role": "Develop advanced analytics and predictive models from design through implementation in the areas of pricing and promotion, marketing, merchandising, and other areas of the business as needed. Participate in the research of analytical methods to find or advance solutions to business problems. Clearly and concisely explain complex analytical findings to non-analytical peers and business leaders. Create analytic datasets in collaboration with data engineering teams that involves data querying, cleaning, transformation, and feature engineering. Define requirements and test cases for Data validation and monitoring. Author programming code (e.g., SQL, Python, R, spark) to assemble and analyze data, following/establishing/enhancing organizational standards and coding practices including code management (i.e. Documentation, use ofGit Hub). Lead and participate in peer code reviews for QA/QC/standards compliance. Train/Mentor other team members, provide developmental support where necessary. Follow defined project management processes. Actively participate in project plan development and agile ceremonies (development of backlog, sprint planning, daily standups, retros/reviews) and documentation.", "Qualification": "Education: Bachelor\u2019s degree required, preferably with a quantitative focus (Statistics, Business Analytics, Data Science, Math, Economics, etc.). Master\u2019s degree preferred. Masters + 1year / Bachelors + 2 years of experience in a data science/advanced analytics role, or demonstrated ability to perform job functions via academic project/internship experience. Knowledge And Skills: Demonstrated knowledge of SQL, R and Python; Experience querying large datasets using SparkSQL or PySpark; Experience with ML frameworks such as scikit-learn, Tensorflow, Keras, Pytorch, etc.; Exposure with software development practices, object-oriented principles and test automation; Exposure to version control systems such as Git \u2013 experience preferred. Experience with cloud-based analytics environments (Azure, AWS or GCP); Experience applying operational research, statistical and machine learning techniques such as regression, time series forecasting, clustering, optimization, etc.; Exposure to agile methodologies using project planning and tracking management tools e.g., JIRA; experience preferred. Strong problem-solving skills and ability to troubleshoot complex distributed systems; Strong interpersonal skills, including the ability to communicate the business benefits of analytics; Availability to travel up to 10% of the time", "Title": "DS"}}, {"Resume": {"Education": "PGDM in Business Analytics\nInstitution: Great Lakes Institute of Management & Illinois Institute of Technology\nDuration: January 2017 \u2013 January 2018\nBachelor of Engineering in Electronics and Communication\nInstitution: New Horizon College of Engineering, Bengaluru (Visvesvaraya Technological University)\nGraduation Date: January 2013", "Experience": "Consultant \u2013 Deloitte USI\nProject 1: Historic Deals Analysis\n\nIndustry: Cross Industry\nDuration: 6 months\nDescription: Conducted end-to-end analysis of historic deals, providing insights for price optimization and process improvement.\nKey Responsibilities:\nExtracted data across geographies.\nBuilt Tableau reports and generated insights.\nTechnical Tools: R, Tableau\nProject 2: Handwriting Recognition\n\nIndustry: Finance\nDuration: 3 months\nDescription: Developed functionalities to convert handwritten images into digital text using LSTM models.\nKey Responsibilities:\nGathered data for English words.\nTrained LSTM models for sentence correction.\nTechnical Tools: Python\nProject 3: Financial Reporting Infrastructure (SWIFT)\n\nIndustry: Finance\nDuration: 8 months\nDescription: Developed analytics infrastructure for financial reporting and expense forecasting on SAP S/4.\nKey Responsibilities:\nDesigned and developed data models.\nBuilt ETL processes and validated reports.\nTechnical Tools: SAP HANA, Tableau, SAP AO\nProject 4: Clinical Healthcare System\n\nIndustry: Healthcare Analytics\nDuration: 2 months\nDescription: Developed analytics infrastructure with advanced analytics capabilities on top of Argus.\nKey Responsibilities:\nDesigned and developed data models.\nBuilt analytical models and validated reports.\nTechnical Tools: Data Modeling, SAP HANA, Tableau, NLP\nProject 5: Consumption-Based Planning (FMCG)\n\nIndustry: FMCG\nDuration: 8 months\nDescription: Implemented CRM and CBP modules for forecasting and performance improvements.\nKey Responsibilities:\nDesigned HANA models and developed data flow for forecasts.\nBuilt customer/sales/funds reporting views.\nTechnical Tools: SAP HANA, BOBJ, Time Series Forecasting\nInternal Initiative: Customer Segmentation & RFM Analysis\n\nIndustry: FMCG\nDuration: 3 months\nDescription: Set up a HANA-Python interface to segment customers and perform advanced analytics.\nKey Achievements:\nSegmented data using K-means and performed RFM analysis.\nDeveloped algorithms to categorize new customers.\nTechnical Tools: Python, HANA\nInternal Initiative: Telecom Invoice State Detection\n\nIndustry: Telecom\nDuration: 1 month\nDescription: Automated invoice state detection using decision trees, reducing manual effort by 60%.\nTechnical Tools: R, SAP PAL, SAP HANA\nSenior Developer \u2013 Accenture\nProject 1: In Process Analytics (IPA)\n\nIndustry: Cross Industry\nDuration: 19 months\nDescription: Developed database objects and data models to provide operational insights using SAP analytics tools.\nKey Responsibilities:\nDesigned SAP HANA models (Attribute, Analytic, Calculation views).\nDeveloped KPIs using complex SQL scripts.\nBuilt dashboards and implemented predictive analytics.\nTechnical Tools: R, SAP HANA, T-SQL\nProject 2: Testing Accelerator for SAP (ATAS)\n\nIndustry: Cross Industry\nDuration: 21 months\nDescription: Led development and deployment of the ATAS tool for database management and analytics.\nKey Responsibilities:\nDesigned database objects (tables, views, stored procedures).\nNormalized database tables and performed performance tuning.\nManaged production and quality database servers.\nTechnical Tools: SQL Server, C#, PL-SQL, T-SQL", "Skills": "Programming & Tools: Python, R, C#, SQL, Tableau, SAP HANA, SAP HANA SQL, SAP HANA PAL, SAP Lumira\nFrameworks & Platforms: Django, Flask\nAdvanced Analytics: Linear Programming, Data Modeling, NLP, Retail Analytics, SCM Analytics, Social Media Analytics\nSpecialized Knowledge: Time Series Forecasting, K-Means Clustering, RFM Analysis", "Projects": "Historic Deals Analysis, Handwriting Recognition, Financial Reporting Infrastructure (SWIFT), Clinical Healthcare System, Consumption-Based Planning (FMCG), Customer Segmentation & RFM Analysis, Telecom Invoice State Detection, In Process Analytics (IPA), Testing Accelerator for SAP (ATAS)"}, "JobDescription": {"Role": "You will be part of the central SAP AI Development Unit and support our customers with the adoption of SAP Business AI solutions, helping them to improve their business results based on latest machine learning technology. In this role you work closely with our development teams and the customer to achieve high value propositions and apply machine learning technology in the daily work of our customers. As a (Senior) Development Project Consultant you guide and steer customer and internal teams to achieve state of the art machine learning solutions.This position requires the ability to work and communicate in a dynamic, interdisciplinary, and intercultural team in an agile and efficient way. As a thought leader in Machine Learning you appreciate the possibility for learning and personal development and therefore you keep track of the latest developments and share your knowledge actively with others. You will be working with and leading teams to successful projects in the context of Machine Learning Technology.", "Qualification": "Degree in Mathematics, Physics, Computer Science, Data Science. Several years relevant work experience in sales or technical oriented customer projects. Experienced in guiding customer interactions up to C Level. Good Overview of Machine Learning Trends and Technology. Experience in implementation of Machine Learning solutions and platforms. Knowledge and experience in Machine Learning Programming Frameworks, like TensorFlow or scikit. Programming Skills, like ABAP, Java, Python, JavaScript, NodeJS, Jupyter Notebook, ABAP. Database Knowledge. Web and Cloud Computing Knowledge. Ideally SAP technology and SAP system integration technology. Fluency in English AND German (written and spoken language).", "Title": "MLE"}}, {"Resume": {"Education": "B.Tech in Computer Science & Engineering\nInstitution: Indo Global College of Engineering, Mohali, Punjab\nGraduation Date: January 2017", "Experience": "Data Science Consultant \u2013 Datamites\nDuration: 24 months\nKey Responsibilities:\nAnalyzed and processed complex datasets using advanced querying, visualization, and analytics tools.\nLoaded, extracted, and validated client data.\nManipulated, cleaned, and processed data using Python.\nDeveloped data visualizations with Tableau.\nData Analyst \u2013 Heretic Solutions Pvt Ltd\nDuration: [Not Specified]\nKey Responsibilities:\nCollaborated with business stakeholders to identify issues and proposed data-driven solutions for decision-making.\nCleaned, processed, and analyzed data using Python, R, and Excel.\nApplied machine learning and statistical techniques to address business problems and generate actionable insights.\nDerived conclusions from raw data and developed tailored recommendations.", "Skills": "Programming & Tools: Python, Tableau, R Studio\nCore Expertise: Data Visualization, Machine Learning, Statistics\nCertifications: IABAC Certified Data Scientist\nSpecial Interests: Advocates for augmented intelligence, with a passion for implementing business ideas in areas like Machine Learning, AI, and Robotics.", "Projects": ""}, "JobDescription": {"Role": "Design, build, and maintain highly scalable, robust, and efficient cloud infrastructure using Google Cloud Platform (GCP) services, including Vertex AI, BigTable, BigQuery, and Cloud Composer. Develop automation and orchestration of ML pipelines, integrating data ingestion, feature engineering, training, and deployment processes. Collaborate with cross-functional teams to understand their needs and build solutions that improve platform usability, scalability, and the overall development experience. Optimize data processing pipelines and cloud resources to ensure low-latency, cost-effective operation. Implement monitoring, alerting, and failover strategies to ensure platform reliability. Stay updated with industry trends and best practices in cloud engineering, data engineering, and machine learning", "Qualification": "Customer-centric mindset: Passionate about delivering an exceptional experience for data scientists through a self-service platform, reducing friction in their workflows. Collaboration: Strong communication skills to work closely with cross-functional teams, including data scientists and engineers, to ensure platform features meet user needs and expectations. Problem-solving: Ability to identify and solve complex technical issues related to ML pipelines, cloud infrastructure, and scalability, ensuring an efficient and robust platform. Automation-first approach: Commitment to streamlining and automating processes for scalability and reliability, enabling data scientists to focus on experimentation and model development. Adaptability: Ability to quickly adjust to new technologies and evolving platform needs to keep the infrastructure cutting-edge and efficient. Ownership and initiative: Comfortable taking ownership of key platform components, driving innovation and improvements that benefit the platform\u2019s scalability and usability. Bachelor\u2019s or Master\u2019s degree in Computer Science, Engineering, or a related field. 2+ years of experience in software engineering with a focus on cloud infrastructure and/or data engineering. Hands-on experience with Google Cloud Platform services such as Vertex AI, BigTable, BigQuery, Cloud Composer, Cloud Storage, etc. Proficiency in one or more programming languages such as Python, Java, and SQL. Experience with orchestration tools such as Apache Airflow (Composer). Knowledge of CI/CD pipelines and DevOps tools for continuous integration and deployment. Familiarity with containerization and orchestration (Docker, Kubernetes). Strong problem-solving skills and attention to detail. Excellent communication skills and ability to work in a collaborative, fast-paced environment", "Title": "SDE"}}, {"Resume": {"Education": "B.Tech in Computer Science & Engineering\nInstitution: Indo Global College of Engineering, Mohali, Punjab\nGraduation Date: January 2017", "Experience": "Data Science Consultant \u2013 Datamites\nDuration: 24 months\nKey Responsibilities:\nAnalyzed and processed complex datasets using advanced querying, visualization, and analytics tools.\nLoaded, extracted, and validated client data.\nManipulated, cleaned, and processed data using Python.\nDeveloped data visualizations with Tableau.\nData Analyst \u2013 Heretic Solutions Pvt Ltd\nDuration: [Not Specified]\nKey Responsibilities:\nCollaborated with business stakeholders to identify issues and proposed data-driven solutions for decision-making.\nCleaned, processed, and analyzed data using Python, R, and Excel.\nApplied machine learning and statistical techniques to address business problems and generate actionable insights.\nDerived conclusions from raw data and developed tailored recommendations.", "Skills": "Programming & Tools: Python, Tableau, R Studio\nCore Expertise: Data Visualization, Machine Learning, Statistics\nCertifications: IABAC Certified Data Scientist\nSpecial Interests: Advocates for augmented intelligence, with a passion for implementing business ideas in areas like Machine Learning, AI, and Robotics.", "Projects": ""}, "JobDescription": {"Role": "Develop advanced analytics and predictive models from design through implementation in the areas of pricing and promotion, marketing, merchandising, and other areas of the business as needed. Participate in the research of analytical methods to find or advance solutions to business problems. Clearly and concisely explain complex analytical findings to non-analytical peers and business leaders. Create analytic datasets in collaboration with data engineering teams that involves data querying, cleaning, transformation, and feature engineering. Define requirements and test cases for Data validation and monitoring. Author programming code (e.g., SQL, Python, R, spark) to assemble and analyze data, following/establishing/enhancing organizational standards and coding practices including code management (i.e. Documentation, use ofGit Hub). Lead and participate in peer code reviews for QA/QC/standards compliance. Train/Mentor other team members, provide developmental support where necessary. Follow defined project management processes. Actively participate in project plan development and agile ceremonies (development of backlog, sprint planning, daily standups, retros/reviews) and documentation.", "Qualification": "Education: Bachelor\u2019s degree required, preferably with a quantitative focus (Statistics, Business Analytics, Data Science, Math, Economics, etc.). Master\u2019s degree preferred. Masters + 1year / Bachelors + 2 years of experience in a data science/advanced analytics role, or demonstrated ability to perform job functions via academic project/internship experience. Knowledge And Skills: Demonstrated knowledge of SQL, R and Python; Experience querying large datasets using SparkSQL or PySpark; Experience with ML frameworks such as scikit-learn, Tensorflow, Keras, Pytorch, etc.; Exposure with software development practices, object-oriented principles and test automation; Exposure to version control systems such as Git \u2013 experience preferred. Experience with cloud-based analytics environments (Azure, AWS or GCP); Experience applying operational research, statistical and machine learning techniques such as regression, time series forecasting, clustering, optimization, etc.; Exposure to agile methodologies using project planning and tracking management tools e.g., JIRA; experience preferred. Strong problem-solving skills and ability to troubleshoot complex distributed systems; Strong interpersonal skills, including the ability to communicate the business benefits of analytics; Availability to travel up to 10% of the time", "Title": "DS"}}, {"Resume": {"Education": "B.Tech in Computer Science & Engineering\nInstitution: Indo Global College of Engineering, Mohali, Punjab\nGraduation Date: January 2017", "Experience": "Data Science Consultant \u2013 Datamites\nDuration: 24 months\nKey Responsibilities:\nAnalyzed and processed complex datasets using advanced querying, visualization, and analytics tools.\nLoaded, extracted, and validated client data.\nManipulated, cleaned, and processed data using Python.\nDeveloped data visualizations with Tableau.\nData Analyst \u2013 Heretic Solutions Pvt Ltd\nDuration: [Not Specified]\nKey Responsibilities:\nCollaborated with business stakeholders to identify issues and proposed data-driven solutions for decision-making.\nCleaned, processed, and analyzed data using Python, R, and Excel.\nApplied machine learning and statistical techniques to address business problems and generate actionable insights.\nDerived conclusions from raw data and developed tailored recommendations.", "Skills": "Programming & Tools: Python, Tableau, R Studio\nCore Expertise: Data Visualization, Machine Learning, Statistics\nCertifications: IABAC Certified Data Scientist\nSpecial Interests: Advocates for augmented intelligence, with a passion for implementing business ideas in areas like Machine Learning, AI, and Robotics.", "Projects": ""}, "JobDescription": {"Role": "You will be part of the central SAP AI Development Unit and support our customers with the adoption of SAP Business AI solutions, helping them to improve their business results based on latest machine learning technology. In this role you work closely with our development teams and the customer to achieve high value propositions and apply machine learning technology in the daily work of our customers. As a (Senior) Development Project Consultant you guide and steer customer and internal teams to achieve state of the art machine learning solutions.This position requires the ability to work and communicate in a dynamic, interdisciplinary, and intercultural team in an agile and efficient way. As a thought leader in Machine Learning you appreciate the possibility for learning and personal development and therefore you keep track of the latest developments and share your knowledge actively with others. You will be working with and leading teams to successful projects in the context of Machine Learning Technology.", "Qualification": "Degree in Mathematics, Physics, Computer Science, Data Science. Several years relevant work experience in sales or technical oriented customer projects. Experienced in guiding customer interactions up to C Level. Good Overview of Machine Learning Trends and Technology. Experience in implementation of Machine Learning solutions and platforms. Knowledge and experience in Machine Learning Programming Frameworks, like TensorFlow or scikit. Programming Skills, like ABAP, Java, Python, JavaScript, NodeJS, Jupyter Notebook, ABAP. Database Knowledge. Web and Cloud Computing Knowledge. Ideally SAP technology and SAP system integration technology. Fluency in English AND German (written and spoken language).", "Title": "MLE"}}, {"Resume": {"Education": "B.Tech in Computer Science Engineering\n\nInstitution: Rayat and Bahra Institute of Engineering and Biotechnology\nGraduation Year: 2016\nPercentage: 78.4%\nHigher Secondary Education\n\nInstitution: Moti Ram Arya Sr. Secondary School\nYear: 2012\nPercentage: 78.4%\nSecondary Education\n\nInstitution: Valley Public School\nYear: 2010\nCGPA: 9.4", "Experience": "Wipro Technologies\nRole: IT Professional (2 Years)\nKey Skills:\nMachine Learning, Deep Learning, Data Science, NLP, Neural Networks\nManaged end-to-end development of software projects, from inception to deployment.\nProficient in Python libraries (Numpy, Pandas, Seaborn, Matplotlib) and cloud environments (GCP).\nDeveloped frameworks with an emphasis on code reusability.\nStrong interpersonal and problem-solving skills with a long-term perspective.", "Skills": "Programming Languages: Python, C++, Java\nLibraries & Frameworks: Numpy, Pandas, Seaborn, Matplotlib, Cufflinks, TensorFlow, Keras\nMachine Learning & AI: Supervised, Unsupervised, and Reinforcement Learning; Algorithms like KNN, Decision Tree, SVM, Logistic Regression, Neural Networks, SGD\nCloud Platforms: Google Cloud Platform (GCP)\nDatabases: MySQL, Oracle\nOperating Systems: Linux, Ubuntu, Windows\nDevelopment Environments: NetBeans, Sublime Text, Jupyter Notebooks\nVisualization Tools: Tableau, Excel, Python Libraries", "Projects": "Wipro Neural Intelligence Platform\n\nTeam Size: 5\nDescription:\nDeveloped a platform leveraging automation and AI technologies, including NLP, cognitive learning, and machine learning analytics. The platform:\nAccesses and manages structured/unstructured data sources.\nIncludes sentiment and predictive analytics.\nFeatures a deep learning engine for continuous learning.\nKey Responsibilities:\nAutomated responses to user queries with \"Monster Bot\" powered by Deep Learning and NLP.\nDeveloped the Entity Extractor for text processing using Regex, TensorFlow, and Bluemix NLU APIs.\nDesigned a Classifier to train datasets and map user queries to optimized responses using SKLearn (MNB, SVM, SGD).\nTrained a deep learning NER model using an RNN (LSTM) bidirectional framework for entity extraction.\nDiabetes Detection Software\n\nAchievements:\nCreated a solution to detect diabetes and received third prize in a competition."}, "JobDescription": {"Role": "Design, build, and maintain highly scalable, robust, and efficient cloud infrastructure using Google Cloud Platform (GCP) services, including Vertex AI, BigTable, BigQuery, and Cloud Composer. Develop automation and orchestration of ML pipelines, integrating data ingestion, feature engineering, training, and deployment processes. Collaborate with cross-functional teams to understand their needs and build solutions that improve platform usability, scalability, and the overall development experience. Optimize data processing pipelines and cloud resources to ensure low-latency, cost-effective operation. Implement monitoring, alerting, and failover strategies to ensure platform reliability. Stay updated with industry trends and best practices in cloud engineering, data engineering, and machine learning", "Qualification": "Customer-centric mindset: Passionate about delivering an exceptional experience for data scientists through a self-service platform, reducing friction in their workflows. Collaboration: Strong communication skills to work closely with cross-functional teams, including data scientists and engineers, to ensure platform features meet user needs and expectations. Problem-solving: Ability to identify and solve complex technical issues related to ML pipelines, cloud infrastructure, and scalability, ensuring an efficient and robust platform. Automation-first approach: Commitment to streamlining and automating processes for scalability and reliability, enabling data scientists to focus on experimentation and model development. Adaptability: Ability to quickly adjust to new technologies and evolving platform needs to keep the infrastructure cutting-edge and efficient. Ownership and initiative: Comfortable taking ownership of key platform components, driving innovation and improvements that benefit the platform\u2019s scalability and usability. Bachelor\u2019s or Master\u2019s degree in Computer Science, Engineering, or a related field. 2+ years of experience in software engineering with a focus on cloud infrastructure and/or data engineering. Hands-on experience with Google Cloud Platform services such as Vertex AI, BigTable, BigQuery, Cloud Composer, Cloud Storage, etc. Proficiency in one or more programming languages such as Python, Java, and SQL. Experience with orchestration tools such as Apache Airflow (Composer). Knowledge of CI/CD pipelines and DevOps tools for continuous integration and deployment. Familiarity with containerization and orchestration (Docker, Kubernetes). Strong problem-solving skills and attention to detail. Excellent communication skills and ability to work in a collaborative, fast-paced environment", "Title": "SDE"}}, {"Resume": {"Education": "B.Tech in Computer Science Engineering\n\nInstitution: Rayat and Bahra Institute of Engineering and Biotechnology\nGraduation Year: 2016\nPercentage: 78.4%\nHigher Secondary Education\n\nInstitution: Moti Ram Arya Sr. Secondary School\nYear: 2012\nPercentage: 78.4%\nSecondary Education\n\nInstitution: Valley Public School\nYear: 2010\nCGPA: 9.4", "Experience": "Wipro Technologies\nRole: IT Professional (2 Years)\nKey Skills:\nMachine Learning, Deep Learning, Data Science, NLP, Neural Networks\nManaged end-to-end development of software projects, from inception to deployment.\nProficient in Python libraries (Numpy, Pandas, Seaborn, Matplotlib) and cloud environments (GCP).\nDeveloped frameworks with an emphasis on code reusability.\nStrong interpersonal and problem-solving skills with a long-term perspective.", "Skills": "Programming Languages: Python, C++, Java\nLibraries & Frameworks: Numpy, Pandas, Seaborn, Matplotlib, Cufflinks, TensorFlow, Keras\nMachine Learning & AI: Supervised, Unsupervised, and Reinforcement Learning; Algorithms like KNN, Decision Tree, SVM, Logistic Regression, Neural Networks, SGD\nCloud Platforms: Google Cloud Platform (GCP)\nDatabases: MySQL, Oracle\nOperating Systems: Linux, Ubuntu, Windows\nDevelopment Environments: NetBeans, Sublime Text, Jupyter Notebooks\nVisualization Tools: Tableau, Excel, Python Libraries", "Projects": "Wipro Neural Intelligence Platform\n\nTeam Size: 5\nDescription:\nDeveloped a platform leveraging automation and AI technologies, including NLP, cognitive learning, and machine learning analytics. The platform:\nAccesses and manages structured/unstructured data sources.\nIncludes sentiment and predictive analytics.\nFeatures a deep learning engine for continuous learning.\nKey Responsibilities:\nAutomated responses to user queries with \"Monster Bot\" powered by Deep Learning and NLP.\nDeveloped the Entity Extractor for text processing using Regex, TensorFlow, and Bluemix NLU APIs.\nDesigned a Classifier to train datasets and map user queries to optimized responses using SKLearn (MNB, SVM, SGD).\nTrained a deep learning NER model using an RNN (LSTM) bidirectional framework for entity extraction.\nDiabetes Detection Software\n\nAchievements:\nCreated a solution to detect diabetes and received third prize in a competition."}, "JobDescription": {"Role": "Develop advanced analytics and predictive models from design through implementation in the areas of pricing and promotion, marketing, merchandising, and other areas of the business as needed. Participate in the research of analytical methods to find or advance solutions to business problems. Clearly and concisely explain complex analytical findings to non-analytical peers and business leaders. Create analytic datasets in collaboration with data engineering teams that involves data querying, cleaning, transformation, and feature engineering. Define requirements and test cases for Data validation and monitoring. Author programming code (e.g., SQL, Python, R, spark) to assemble and analyze data, following/establishing/enhancing organizational standards and coding practices including code management (i.e. Documentation, use ofGit Hub). Lead and participate in peer code reviews for QA/QC/standards compliance. Train/Mentor other team members, provide developmental support where necessary. Follow defined project management processes. Actively participate in project plan development and agile ceremonies (development of backlog, sprint planning, daily standups, retros/reviews) and documentation.", "Qualification": "Education: Bachelor\u2019s degree required, preferably with a quantitative focus (Statistics, Business Analytics, Data Science, Math, Economics, etc.). Master\u2019s degree preferred. Masters + 1year / Bachelors + 2 years of experience in a data science/advanced analytics role, or demonstrated ability to perform job functions via academic project/internship experience. Knowledge And Skills: Demonstrated knowledge of SQL, R and Python; Experience querying large datasets using SparkSQL or PySpark; Experience with ML frameworks such as scikit-learn, Tensorflow, Keras, Pytorch, etc.; Exposure with software development practices, object-oriented principles and test automation; Exposure to version control systems such as Git \u2013 experience preferred. Experience with cloud-based analytics environments (Azure, AWS or GCP); Experience applying operational research, statistical and machine learning techniques such as regression, time series forecasting, clustering, optimization, etc.; Exposure to agile methodologies using project planning and tracking management tools e.g., JIRA; experience preferred. Strong problem-solving skills and ability to troubleshoot complex distributed systems; Strong interpersonal skills, including the ability to communicate the business benefits of analytics; Availability to travel up to 10% of the time", "Title": "DS"}}, {"Resume": {"Education": "B.Tech in Computer Science Engineering\n\nInstitution: Rayat and Bahra Institute of Engineering and Biotechnology\nGraduation Year: 2016\nPercentage: 78.4%\nHigher Secondary Education\n\nInstitution: Moti Ram Arya Sr. Secondary School\nYear: 2012\nPercentage: 78.4%\nSecondary Education\n\nInstitution: Valley Public School\nYear: 2010\nCGPA: 9.4", "Experience": "Wipro Technologies\nRole: IT Professional (2 Years)\nKey Skills:\nMachine Learning, Deep Learning, Data Science, NLP, Neural Networks\nManaged end-to-end development of software projects, from inception to deployment.\nProficient in Python libraries (Numpy, Pandas, Seaborn, Matplotlib) and cloud environments (GCP).\nDeveloped frameworks with an emphasis on code reusability.\nStrong interpersonal and problem-solving skills with a long-term perspective.", "Skills": "Programming Languages: Python, C++, Java\nLibraries & Frameworks: Numpy, Pandas, Seaborn, Matplotlib, Cufflinks, TensorFlow, Keras\nMachine Learning & AI: Supervised, Unsupervised, and Reinforcement Learning; Algorithms like KNN, Decision Tree, SVM, Logistic Regression, Neural Networks, SGD\nCloud Platforms: Google Cloud Platform (GCP)\nDatabases: MySQL, Oracle\nOperating Systems: Linux, Ubuntu, Windows\nDevelopment Environments: NetBeans, Sublime Text, Jupyter Notebooks\nVisualization Tools: Tableau, Excel, Python Libraries", "Projects": "Wipro Neural Intelligence Platform\n\nTeam Size: 5\nDescription:\nDeveloped a platform leveraging automation and AI technologies, including NLP, cognitive learning, and machine learning analytics. The platform:\nAccesses and manages structured/unstructured data sources.\nIncludes sentiment and predictive analytics.\nFeatures a deep learning engine for continuous learning.\nKey Responsibilities:\nAutomated responses to user queries with \"Monster Bot\" powered by Deep Learning and NLP.\nDeveloped the Entity Extractor for text processing using Regex, TensorFlow, and Bluemix NLU APIs.\nDesigned a Classifier to train datasets and map user queries to optimized responses using SKLearn (MNB, SVM, SGD).\nTrained a deep learning NER model using an RNN (LSTM) bidirectional framework for entity extraction.\nDiabetes Detection Software\n\nAchievements:\nCreated a solution to detect diabetes and received third prize in a competition."}, "JobDescription": {"Role": "You will be part of the central SAP AI Development Unit and support our customers with the adoption of SAP Business AI solutions, helping them to improve their business results based on latest machine learning technology. In this role you work closely with our development teams and the customer to achieve high value propositions and apply machine learning technology in the daily work of our customers. As a (Senior) Development Project Consultant you guide and steer customer and internal teams to achieve state of the art machine learning solutions.This position requires the ability to work and communicate in a dynamic, interdisciplinary, and intercultural team in an agile and efficient way. As a thought leader in Machine Learning you appreciate the possibility for learning and personal development and therefore you keep track of the latest developments and share your knowledge actively with others. You will be working with and leading teams to successful projects in the context of Machine Learning Technology.", "Qualification": "Degree in Mathematics, Physics, Computer Science, Data Science. Several years relevant work experience in sales or technical oriented customer projects. Experienced in guiding customer interactions up to C Level. Good Overview of Machine Learning Trends and Technology. Experience in implementation of Machine Learning solutions and platforms. Knowledge and experience in Machine Learning Programming Frameworks, like TensorFlow or scikit. Programming Skills, like ABAP, Java, Python, JavaScript, NodeJS, Jupyter Notebook, ABAP. Database Knowledge. Web and Cloud Computing Knowledge. Ideally SAP technology and SAP system integration technology. Fluency in English AND German (written and spoken language).", "Title": "MLE"}}, {"Resume": {"Education": "B.Tech in Computer Science Engineering\n\nInstitution: Rayat and Bahra Institute of Engineering and Biotechnology\nGraduation Year: 2016\nPercentage: 78.4%\nHigher Secondary Education\n\nInstitution: Moti Ram Arya Sr. Secondary School\nYear: 2012\nPercentage: 78.4%\nSecondary Education\n\nInstitution: Valley Public School\nYear: 2010\nCGPA: 9.4", "Experience": "Wipro Technologies\nRole: IT Professional (2 Years)\nKey Skills:\nMachine Learning, Deep Learning, Data Science, NLP, Neural Networks\nManaged end-to-end development of software projects, from inception to deployment.\nProficient in Python libraries (Numpy, Pandas, Seaborn, Matplotlib) and cloud environments (GCP).\nDeveloped frameworks with an emphasis on code reusability.\nStrong interpersonal and problem-solving skills with a long-term perspective.", "Skills": "Programming Languages: Python, C++, Java\nLibraries & Frameworks: Numpy, Pandas, Seaborn, Matplotlib, Cufflinks, TensorFlow, Keras\nMachine Learning & AI: Supervised, Unsupervised, and Reinforcement Learning; Algorithms like KNN, Decision Tree, SVM, Logistic Regression, Neural Networks, SGD\nCloud Platforms: Google Cloud Platform (GCP)\nDatabases: MySQL, Oracle\nOperating Systems: Linux, Ubuntu, Windows\nDevelopment Environments: NetBeans, Sublime Text, Jupyter Notebooks\nVisualization Tools: Tableau, Excel, Python Libraries", "Projects": "Wipro Neural Intelligence Platform\n\nTeam Size: 5\nDescription:\nDeveloped a platform leveraging automation and AI technologies, including NLP, cognitive learning, and machine learning analytics. The platform:\nAccesses and manages structured/unstructured data sources.\nIncludes sentiment and predictive analytics.\nFeatures a deep learning engine for continuous learning.\nKey Responsibilities:\nAutomated responses to user queries with \"Monster Bot\" powered by Deep Learning and NLP.\nDeveloped the Entity Extractor for text processing using Regex, TensorFlow, and Bluemix NLU APIs.\nDesigned a Classifier to train datasets and map user queries to optimized responses using SKLearn (MNB, SVM, SGD).\nTrained a deep learning NER model using an RNN (LSTM) bidirectional framework for entity extraction.\nDiabetes Detection Software\n\nAchievements:\nCreated a solution to detect diabetes and received third prize in a competition."}, "JobDescription": {"Role": "Design, build, and maintain highly scalable, robust, and efficient cloud infrastructure using Google Cloud Platform (GCP) services, including Vertex AI, BigTable, BigQuery, and Cloud Composer. Develop automation and orchestration of ML pipelines, integrating data ingestion, feature engineering, training, and deployment processes. Collaborate with cross-functional teams to understand their needs and build solutions that improve platform usability, scalability, and the overall development experience. Optimize data processing pipelines and cloud resources to ensure low-latency, cost-effective operation. Implement monitoring, alerting, and failover strategies to ensure platform reliability. Stay updated with industry trends and best practices in cloud engineering, data engineering, and machine learning", "Qualification": "Customer-centric mindset: Passionate about delivering an exceptional experience for data scientists through a self-service platform, reducing friction in their workflows. Collaboration: Strong communication skills to work closely with cross-functional teams, including data scientists and engineers, to ensure platform features meet user needs and expectations. Problem-solving: Ability to identify and solve complex technical issues related to ML pipelines, cloud infrastructure, and scalability, ensuring an efficient and robust platform. Automation-first approach: Commitment to streamlining and automating processes for scalability and reliability, enabling data scientists to focus on experimentation and model development. Adaptability: Ability to quickly adjust to new technologies and evolving platform needs to keep the infrastructure cutting-edge and efficient. Ownership and initiative: Comfortable taking ownership of key platform components, driving innovation and improvements that benefit the platform\u2019s scalability and usability. Bachelor\u2019s or Master\u2019s degree in Computer Science, Engineering, or a related field. 2+ years of experience in software engineering with a focus on cloud infrastructure and/or data engineering. Hands-on experience with Google Cloud Platform services such as Vertex AI, BigTable, BigQuery, Cloud Composer, Cloud Storage, etc. Proficiency in one or more programming languages such as Python, Java, and SQL. Experience with orchestration tools such as Apache Airflow (Composer). Knowledge of CI/CD pipelines and DevOps tools for continuous integration and deployment. Familiarity with containerization and orchestration (Docker, Kubernetes). Strong problem-solving skills and attention to detail. Excellent communication skills and ability to work in a collaborative, fast-paced environment", "Title": "SDE"}}, {"Resume": {"Education": "B.Tech in Computer Science Engineering\n\nInstitution: Rayat and Bahra Institute of Engineering and Biotechnology\nGraduation Year: 2016\nPercentage: 78.4%\nHigher Secondary Education\n\nInstitution: Moti Ram Arya Sr. Secondary School\nYear: 2012\nPercentage: 78.4%\nSecondary Education\n\nInstitution: Valley Public School\nYear: 2010\nCGPA: 9.4", "Experience": "Wipro Technologies\nRole: IT Professional (2 Years)\nKey Skills:\nMachine Learning, Deep Learning, Data Science, NLP, Neural Networks\nManaged end-to-end development of software projects, from inception to deployment.\nProficient in Python libraries (Numpy, Pandas, Seaborn, Matplotlib) and cloud environments (GCP).\nDeveloped frameworks with an emphasis on code reusability.\nStrong interpersonal and problem-solving skills with a long-term perspective.", "Skills": "Programming Languages: Python, C++, Java\nLibraries & Frameworks: Numpy, Pandas, Seaborn, Matplotlib, Cufflinks, TensorFlow, Keras\nMachine Learning & AI: Supervised, Unsupervised, and Reinforcement Learning; Algorithms like KNN, Decision Tree, SVM, Logistic Regression, Neural Networks, SGD\nCloud Platforms: Google Cloud Platform (GCP)\nDatabases: MySQL, Oracle\nOperating Systems: Linux, Ubuntu, Windows\nDevelopment Environments: NetBeans, Sublime Text, Jupyter Notebooks\nVisualization Tools: Tableau, Excel, Python Libraries", "Projects": "Wipro Neural Intelligence Platform\n\nTeam Size: 5\nDescription:\nDeveloped a platform leveraging automation and AI technologies, including NLP, cognitive learning, and machine learning analytics. The platform:\nAccesses and manages structured/unstructured data sources.\nIncludes sentiment and predictive analytics.\nFeatures a deep learning engine for continuous learning.\nKey Responsibilities:\nAutomated responses to user queries with \"Monster Bot\" powered by Deep Learning and NLP.\nDeveloped the Entity Extractor for text processing using Regex, TensorFlow, and Bluemix NLU APIs.\nDesigned a Classifier to train datasets and map user queries to optimized responses using SKLearn (MNB, SVM, SGD).\nTrained a deep learning NER model using an RNN (LSTM) bidirectional framework for entity extraction.\nDiabetes Detection Software\n\nAchievements:\nCreated a solution to detect diabetes and received third prize in a competition."}, "JobDescription": {"Role": "Develop advanced analytics and predictive models from design through implementation in the areas of pricing and promotion, marketing, merchandising, and other areas of the business as needed. Participate in the research of analytical methods to find or advance solutions to business problems. Clearly and concisely explain complex analytical findings to non-analytical peers and business leaders. Create analytic datasets in collaboration with data engineering teams that involves data querying, cleaning, transformation, and feature engineering. Define requirements and test cases for Data validation and monitoring. Author programming code (e.g., SQL, Python, R, spark) to assemble and analyze data, following/establishing/enhancing organizational standards and coding practices including code management (i.e. Documentation, use ofGit Hub). Lead and participate in peer code reviews for QA/QC/standards compliance. Train/Mentor other team members, provide developmental support where necessary. Follow defined project management processes. Actively participate in project plan development and agile ceremonies (development of backlog, sprint planning, daily standups, retros/reviews) and documentation.", "Qualification": "Education: Bachelor\u2019s degree required, preferably with a quantitative focus (Statistics, Business Analytics, Data Science, Math, Economics, etc.). Master\u2019s degree preferred. Masters + 1year / Bachelors + 2 years of experience in a data science/advanced analytics role, or demonstrated ability to perform job functions via academic project/internship experience. Knowledge And Skills: Demonstrated knowledge of SQL, R and Python; Experience querying large datasets using SparkSQL or PySpark; Experience with ML frameworks such as scikit-learn, Tensorflow, Keras, Pytorch, etc.; Exposure with software development practices, object-oriented principles and test automation; Exposure to version control systems such as Git \u2013 experience preferred. Experience with cloud-based analytics environments (Azure, AWS or GCP); Experience applying operational research, statistical and machine learning techniques such as regression, time series forecasting, clustering, optimization, etc.; Exposure to agile methodologies using project planning and tracking management tools e.g., JIRA; experience preferred. Strong problem-solving skills and ability to troubleshoot complex distributed systems; Strong interpersonal skills, including the ability to communicate the business benefits of analytics; Availability to travel up to 10% of the time", "Title": "DS"}}, {"Resume": {"Education": "B.Tech in Computer Science Engineering\n\nInstitution: Rayat and Bahra Institute of Engineering and Biotechnology\nGraduation Year: 2016\nPercentage: 78.4%\nHigher Secondary Education\n\nInstitution: Moti Ram Arya Sr. Secondary School\nYear: 2012\nPercentage: 78.4%\nSecondary Education\n\nInstitution: Valley Public School\nYear: 2010\nCGPA: 9.4", "Experience": "Wipro Technologies\nRole: IT Professional (2 Years)\nKey Skills:\nMachine Learning, Deep Learning, Data Science, NLP, Neural Networks\nManaged end-to-end development of software projects, from inception to deployment.\nProficient in Python libraries (Numpy, Pandas, Seaborn, Matplotlib) and cloud environments (GCP).\nDeveloped frameworks with an emphasis on code reusability.\nStrong interpersonal and problem-solving skills with a long-term perspective.", "Skills": "Programming Languages: Python, C++, Java\nLibraries & Frameworks: Numpy, Pandas, Seaborn, Matplotlib, Cufflinks, TensorFlow, Keras\nMachine Learning & AI: Supervised, Unsupervised, and Reinforcement Learning; Algorithms like KNN, Decision Tree, SVM, Logistic Regression, Neural Networks, SGD\nCloud Platforms: Google Cloud Platform (GCP)\nDatabases: MySQL, Oracle\nOperating Systems: Linux, Ubuntu, Windows\nDevelopment Environments: NetBeans, Sublime Text, Jupyter Notebooks\nVisualization Tools: Tableau, Excel, Python Libraries", "Projects": "Wipro Neural Intelligence Platform\n\nTeam Size: 5\nDescription:\nDeveloped a platform leveraging automation and AI technologies, including NLP, cognitive learning, and machine learning analytics. The platform:\nAccesses and manages structured/unstructured data sources.\nIncludes sentiment and predictive analytics.\nFeatures a deep learning engine for continuous learning.\nKey Responsibilities:\nAutomated responses to user queries with \"Monster Bot\" powered by Deep Learning and NLP.\nDeveloped the Entity Extractor for text processing using Regex, TensorFlow, and Bluemix NLU APIs.\nDesigned a Classifier to train datasets and map user queries to optimized responses using SKLearn (MNB, SVM, SGD).\nTrained a deep learning NER model using an RNN (LSTM) bidirectional framework for entity extraction.\nDiabetes Detection Software\n\nAchievements:\nCreated a solution to detect diabetes and received third prize in a competition."}, "JobDescription": {"Role": "You will be part of the central SAP AI Development Unit and support our customers with the adoption of SAP Business AI solutions, helping them to improve their business results based on latest machine learning technology. In this role you work closely with our development teams and the customer to achieve high value propositions and apply machine learning technology in the daily work of our customers. As a (Senior) Development Project Consultant you guide and steer customer and internal teams to achieve state of the art machine learning solutions.This position requires the ability to work and communicate in a dynamic, interdisciplinary, and intercultural team in an agile and efficient way. As a thought leader in Machine Learning you appreciate the possibility for learning and personal development and therefore you keep track of the latest developments and share your knowledge actively with others. You will be working with and leading teams to successful projects in the context of Machine Learning Technology.", "Qualification": "Degree in Mathematics, Physics, Computer Science, Data Science. Several years relevant work experience in sales or technical oriented customer projects. Experienced in guiding customer interactions up to C Level. Good Overview of Machine Learning Trends and Technology. Experience in implementation of Machine Learning solutions and platforms. Knowledge and experience in Machine Learning Programming Frameworks, like TensorFlow or scikit. Programming Skills, like ABAP, Java, Python, JavaScript, NodeJS, Jupyter Notebook, ABAP. Database Knowledge. Web and Cloud Computing Knowledge. Ideally SAP technology and SAP system integration technology. Fluency in English AND German (written and spoken language).", "Title": "MLE"}}]